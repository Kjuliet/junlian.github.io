<!DOCTYPE html>
<html lang="en">
<head><meta name="generator" content="Hexo 3.9.0">
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="google-site-verification" content="xBT4GhYoi5qRD5tr338pgPM5OWHHIDR6mNg1a3euekI">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Juliet&#39;s blog">
    <meta name="keyword" content>
    <link rel="shortcut icon" href="/junlian.github.io/img/favicon.ico">

    <title>
        
        Batch Normalization (ICML, 2015) - 学 | 慢慢来
        
    </title>

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/junlian.github.io/css/aircloud.css">
    <link rel="stylesheet" href="/junlian.github.io/css/gitment.css">
    <!--<link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css">-->
    <link href="//at.alicdn.com/t/font_620856_pl6z7sid89qkt9.css" rel="stylesheet" type="text/css">
    <!-- ga & ba script hoook -->
    <script></script>
</head>

<body>

<div class="site-nav-toggle" id="site-nav-toggle">
    <button>
        <span class="btn-bar"></span>
        <span class="btn-bar"></span>
        <span class="btn-bar"></span>
    </button>
</div>

<div class="index-about">
    <i> 所谓诚其意者，毋自欺也，如恶恶臭，如好好色，此之谓自谦，故君子必慎其独也。 ——摘《大学》 </i>
</div>

<div class="index-container">
    
    <div class="index-left">
        
<div class="nav" id="nav">
    <div class="avatar-name">
        <div class="avatar ">
            <img src="/junlian.github.io/img/avatar.jpg" />
        </div>
        <div class="name">
            <i>慎思之，笃行之</i>
        </div>
    </div>
    <div class="contents" id="nav-content">
        <ul>
            <li >
                <a href="/junlian.github.io/">
                    <i class="iconfont icon-shouye1"></i>
                    <span>主页</span>
                </a>
            </li>
            <li >
                <a href="/junlian.github.io/tags">
                    <i class="iconfont icon-biaoqian1"></i>
                    <span>标签</span>
                </a>
            </li>
            <li >
                <a href="/junlian.github.io/archives">
                    <i class="iconfont icon-guidang2"></i>
                    <span>存档</span>
                </a>
            </li>
            <li >
                <a href="/junlian.github.io/about/">
                    <i class="iconfont icon-guanyu2"></i>
                    <span>关于</span>
                </a>
            </li>
            
            <li>
                <a id="search">
                    <i class="iconfont icon-sousuo1"></i>
                    <span>搜索</span>
                </a>
            </li>
            
        </ul>
    </div>
    
        <div id="toc" class="toc-article">
    <ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-Motivation"><span class="toc-text">1. Motivation</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-1-梯度弥散"><span class="toc-text">1.1 梯度弥散</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-2-ICS-Internal-Covariate-Shift"><span class="toc-text">1.2 ICS (Internal Covariate Shift)</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-BN-用法"><span class="toc-text">2. BN 用法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-1-Shift-和-scale"><span class="toc-text">2.1 Shift 和 scale</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-BN-训练过程"><span class="toc-text">2.2 BN 训练过程</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-BN测试"><span class="toc-text">2.3 BN测试</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-使用场景"><span class="toc-text">3. 使用场景</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-使用细节"><span class="toc-text">4. 使用细节</span></a></li></ol>
</div>
    
</div>


<div class="search-field" id="search-field">
    <div class="search-container">
        <div class="search-input">
            <span id="esc-search"> <i class="icon-fanhui iconfont"></i></span>
            <input id="search-input"/>
            <span id="begin-search">搜索</span>
        </div>
        <div class="search-result-container" id="search-result-container">

        </div>
    </div>
</div>

        <div class="index-about-mobile">
            <i> 所谓诚其意者，毋自欺也，如恶恶臭，如好好色，此之谓自谦，故君子必慎其独也。 ——摘《大学》 </i>
        </div>
    </div>
    
    <div class="index-middle">
        <!-- Main Content -->
        


<div class="post-container">
    <div class="post-title">
        Batch Normalization (ICML, 2015)
    </div>

    <div class="post-meta">
        <span class="attr">发布于：<span>2019-08-26 11:02:46</span></span>
        
        </span>
        <span class="attr">访问：<span id="busuanzi_value_page_pv"></span>
</span>
</span>
    </div>
    <div class="post-content no-indent">
        <p>Batch Normalization就是通过用 minibatch 对 activation 做规范化操作，让输出的均值为0， 方差为1，还可以计算原输入的均值和方差 $\gamma^{(k)}=\sqrt{Var[x^{(k)}]}$， $\beta^{(k)} = E[x^{(k)}]$ 进行 scale 和 shift，让BN还原最初的输入。</p>
<p>加入BN提高了模型的capacity：BN 是在原模型上加入的“新操作”，这个新操作很大可能会改变某层的输入，也可能不改变，即“还原原来的输入”，也就是说BN既可以改变也可以保持原输入，所以模型的容纳能力 capacity 提高了。</p>
<h3 id="1-Motivation"><a href="#1-Motivation" class="headerlink" title="1. Motivation"></a>1. Motivation</h3><h4 id="1-1-梯度弥散"><a href="#1-1-梯度弥散" class="headerlink" title="1.1 梯度弥散"></a>1.1 梯度弥散</h4><p><strong>BN 实际上防止了梯度弥散，也就是 $0.9^{30} = 0.04$，BN 通过将activation规范为一致的均值和方差，使原本会减少的 activation 的 scale 变大，是一种更有效的 local response normalization 的方法。</strong></p>
<p>反向传播时中间某一层的梯度计算：</p>
<script type="math/tex; mode=display">\frac{\partial l}{\partial h_{l-1}} = \frac{\partial l}{\partial h_l} \cdot \frac{\partial h_l}{\partial h_{l-1}} = \frac{\partial l}{\partial h_l} w_l</script><p>从l层传到k层：</p>
<script type="math/tex; mode=display">\frac{\partial l}{\partial h_k} = \frac{\partial l}{\partial _l}\prod \limits_{i=k+1}^l w_i</script><p>$\prod \limits_{i=k+1}^l w_i$ 意味着，随着网络层数的加深，如果$w_i$ 大多小于1，那么传到前面梯度会变得非常小 $0.9^{30} = 0.04$，如果 $w_i$大多都大于1，传到前面就会有梯度爆炸问题。BN 起到的作用就是抹去了 w 的scale 影响。</p>
<h4 id="1-2-ICS-Internal-Covariate-Shift"><a href="#1-2-ICS-Internal-Covariate-Shift" class="headerlink" title="1.2 ICS (Internal Covariate Shift)"></a>1.2 ICS (Internal Covariate Shift)</h4><p>论文认为通过 minibatch来规范化某些层的输入，固定每层输入的均值和方差，可以解决covariate shift问题。</p>
<p>机器学习中假设 源空间source domain 和 目标空间 target domain 的数据分布是一致的，不一致的情况就是 transfer learning 和 domain adaptation 要解决的问题，Covariate Shift 就是“分布不一致”假设下的一个分支问题，它是指源空间和目标空间的条件概率是一致的，但边缘概率不同，即对所有 $x\in X, P_s(Y|X=x) = P_t(Y|X=x)$，$P_s(X) \ne P_t(X)$。</p>
<p>对于神经网络经过层内操作后，各层输出的分布显然和各层对应的输入分布不同，而且差异会随着网络层数的加深而增大，而他们代表的样本标记label是不变的，所以符合 covariate shift 的定义。因为是对层间信号分析，所以叫 internal。</p>
<h3 id="2-BN-用法"><a href="#2-BN-用法" class="headerlink" title="2. BN 用法"></a>2. BN 用法</h3><h4 id="2-1-Shift-和-scale"><a href="#2-1-Shift-和-scale" class="headerlink" title="2.1 Shift 和 scale"></a>2.1 Shift 和 scale</h4><p>如果把每一层输出后的数据都归一化到0均值，1方差，每一层都是标准正态分布，导致网络从输入数据好不容易学到的特征被归一化了，所以加入了可训练的归一化参数 $\beta$ 和 $\gamma$。</p>
<h4 id="2-2-BN-训练过程"><a href="#2-2-BN-训练过程" class="headerlink" title="2.2 BN 训练过程"></a>2.2 BN 训练过程</h4><p>BN 是直接对输入的每个维度独立做规范化，用在每个minibatch中计算得到的均值和方差，代表整个训练集的均值和方差。使用时是加在网络的 activation set 上，用在非线性映射前，是对 $z=Wx + b$ 做规范化：</p>
<p><strong>输入：</strong>一个minibatch 的 x：$B=\{x_1, x_2, …, x_m\}$<br><strong>参数：</strong>$\gamma$, $\beta$<br><strong>输出：</strong>$\{y_i = BN_{\gamma\beta}(x_i)\}$</p>
<script type="math/tex; mode=display">
\begin{array}{ll}
\mu_B = \frac{1}{m}\sum\limits_{i=1}^m x_i \\
\sigma_B^2 = \frac{1}{m}\sum\limits_{i=1}^m (x_i - \mu_{B})^2 \\
\hat x_i = \frac{x_i - \mu_{B}}{\sqrt{\sigma_B^2 + \epsilon}} \\
y_i = \gamma \hat x_i + \beta 
\end{array}</script><p><strong>反向传播过程</strong></p>
<script type="math/tex; mode=display">
\begin{array}{ll}
\frac{\partial l}{\partial x_i} = \frac{\partial l}{\partial y_i} \cdot \gamma \\
\frac{\partial l}{\partial \sigma_B^2} = \sum_{i=1}^{m}\frac{\partial l}{\partial \hat x_i}\cdot (x_i - \mu_B) \cdot -\frac{1}{2}(\sigma_B^2 +\epsilon)^{-\frac{3}{2}} \\
\frac{\partial l}{\partial \mu _B} = \sum_{i=1}^m \frac{\partial l}{\partial \hat x_i} \cdot \frac{-1}{\sqrt{\sigma_B^2 + \epsilon}} \\
\frac{\partial l}{\partial x_i} = \frac{\partial l}{\partial \hat x_i} \cdot \frac{1}{\sqrt{\sigma_B^2 + \epsilon}} + \frac{\partial l}{\partial \sigma_B^2} \cdot \frac{2(x_i - \mu_B)}{m} + \frac{\partial l}{\partial \mu_B} \cdot \frac{1}{m} \\
\frac{\partial l}{\partial \gamma} = \sum_{i=1}^m \frac{\partial l}{\partial y_i} \cdot \hat x_i \\
\frac{\partial l}{\partial \beta} = \sum _{i=1}^m \frac{\partial l}{\partial y_i}
\end{array}</script><h4 id="2-3-BN测试"><a href="#2-3-BN测试" class="headerlink" title="2.3 BN测试"></a>2.3 BN测试</h4><p>测试时均值、方差、$\gamma$、$\beta$ 都是固定的，其中均值和方差是训练的时候统计出来的，$\gamma$、$\beta$ 是学出来的。<br>神经网络架构搜索 NAS 有时会finetune bn。</p>
<h3 id="3-使用场景"><a href="#3-使用场景" class="headerlink" title="3. 使用场景"></a>3. 使用场景</h3><p>当出现以下问题时，可以考虑用 BN：</p>
<ul>
<li>训练时收敛速度很慢；</li>
<li>出现梯度爆炸；</li>
<li>加入BN 加快训练速度，提高模型精度。</li>
</ul>
<h3 id="4-使用细节"><a href="#4-使用细节" class="headerlink" title="4. 使用细节"></a>4. 使用细节</h3><ul>
<li>通过移动平均来平滑均值和方差，平滑系数默认0.999，一般0.9</li>
<li>为防止方差分母为0，会在分母上加一个很小的值，一般1e-05</li>
<li>scale和shift操作，对每个元素做ElementwiseAffine: y=kx+b, k，b是要训练的参数，可以选择在channel还是在global上做，一般shared_in_channels = False, 通道之间不共享同一个scaling 和bias</li>
</ul>

        
        <br />
        <div id="comment-container">
        </div>
        <div id="disqus_thread"></div>

        <div id="lv-container">
        </div>

    </div>
</div>

    </div>
</div>


<footer class="footer">
    <ul class="list-inline text-center">
        
        

        

        

        

        

    </ul>
    
    <p>
        <span id="busuanzi_container_site_pv">
            <span id="busuanzi_value_site_pv"></span>PV
        </span>
        <span id="busuanzi_container_site_uv">
            <span id="busuanzi_value_site_uv"></span>UV
        </span>
        Created By <a href="https://hexo.io/">Hexo</a>  Theme <a href="https://github.com/aircloud/hexo-theme-aircloud">AirCloud</a></p>
</footer>




<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });

</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<!-- <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> -->

</body>

<script>
    // We expose some of the variables needed by the front end
    window.hexo_search_path = "search.json"
    window.hexo_root = "/junlian.github.io/"
    window.isPost = true
</script>
<script src="https://cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script>
<script src="/junlian.github.io/js/index.js"></script>
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




</html>
