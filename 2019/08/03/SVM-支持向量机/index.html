<!DOCTYPE html>
<html lang="en">
<head><meta name="generator" content="Hexo 3.9.0">
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="google-site-verification" content="xBT4GhYoi5qRD5tr338pgPM5OWHHIDR6mNg1a3euekI">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Juliet&#39;s blog">
    <meta name="keyword" content>
    <link rel="shortcut icon" href="/junlian.github.io/img/favicon.ico">

    <title>
        
        SVM 支持向量机 - 学 | 慢慢来
        
    </title>

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/junlian.github.io/css/aircloud.css">
    <link rel="stylesheet" href="/junlian.github.io/css/gitment.css">
    <!--<link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css">-->
    <link href="//at.alicdn.com/t/font_620856_pl6z7sid89qkt9.css" rel="stylesheet" type="text/css">
    <!-- ga & ba script hoook -->
    <script></script>
</head>

<body>

<div class="site-nav-toggle" id="site-nav-toggle">
    <button>
        <span class="btn-bar"></span>
        <span class="btn-bar"></span>
        <span class="btn-bar"></span>
    </button>
</div>

<div class="index-about">
    <i> 所谓诚其意者，毋自欺也，如恶恶臭，如好好色，此之谓自谦，故君子必慎其独也。 ——摘《大学》 </i>
</div>

<div class="index-container">
    
    <div class="index-left">
        
<div class="nav" id="nav">
    <div class="avatar-name">
        <div class="avatar ">
            <img src="/junlian.github.io/img/avatar.jpg" />
        </div>
        <div class="name">
            <i>慎思之，笃行之</i>
        </div>
    </div>
    <div class="contents" id="nav-content">
        <ul>
            <li >
                <a href="/junlian.github.io/">
                    <i class="iconfont icon-shouye1"></i>
                    <span>主页</span>
                </a>
            </li>
            <li >
                <a href="/junlian.github.io/tags">
                    <i class="iconfont icon-biaoqian1"></i>
                    <span>标签</span>
                </a>
            </li>
            <li >
                <a href="/junlian.github.io/archives">
                    <i class="iconfont icon-guidang2"></i>
                    <span>存档</span>
                </a>
            </li>
            <li >
                <a href="/junlian.github.io/about/">
                    <i class="iconfont icon-guanyu2"></i>
                    <span>关于</span>
                </a>
            </li>
            
            <li>
                <a id="search">
                    <i class="iconfont icon-sousuo1"></i>
                    <span>搜索</span>
                </a>
            </li>
            
        </ul>
    </div>
    
        <div id="toc" class="toc-article">
    <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-线性可分支持向量机"><span class="toc-text">1 线性可分支持向量机</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-定义"><span class="toc-text">1.1 定义</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-函数间隔和几何间隔"><span class="toc-text">1.2 函数间隔和几何间隔</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-最大间隔法"><span class="toc-text">1.3 最大间隔法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-4-对偶算法"><span class="toc-text">1.4 对偶算法</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-线性支持向量机"><span class="toc-text">2 线性支持向量机</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-线性支持向量机"><span class="toc-text">2.1 线性支持向量机</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-学习的对偶算法"><span class="toc-text">2.2 学习的对偶算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-支持向量"><span class="toc-text">2.3 支持向量</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4-合页损失函数"><span class="toc-text">2.4 合页损失函数</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-非线性支持向量机与核函数"><span class="toc-text">3. 非线性支持向量机与核函数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-核技巧"><span class="toc-text">3.1 核技巧</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1-1-非线性分类问题"><span class="toc-text">3.1.1 非线性分类问题</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1-2-核函数"><span class="toc-text">3.1.2 核函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1-3-核技巧在支持向量机中应用"><span class="toc-text">3.1.3 核技巧在支持向量机中应用</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-正定核"><span class="toc-text">3.2 正定核</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-常用核函数"><span class="toc-text">3.3 常用核函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-非线性支持向量机"><span class="toc-text">3.4 非线性支持向量机</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-序列最小最优化算法"><span class="toc-text">4 序列最小最优化算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#SMO-算法"><span class="toc-text">SMO 算法</span></a></li></ol></li></ol>
</div>
    
</div>


<div class="search-field" id="search-field">
    <div class="search-container">
        <div class="search-input">
            <span id="esc-search"> <i class="icon-fanhui iconfont"></i></span>
            <input id="search-input"/>
            <span id="begin-search">搜索</span>
        </div>
        <div class="search-result-container" id="search-result-container">

        </div>
    </div>
</div>

        <div class="index-about-mobile">
            <i> 所谓诚其意者，毋自欺也，如恶恶臭，如好好色，此之谓自谦，故君子必慎其独也。 ——摘《大学》 </i>
        </div>
    </div>
    
    <div class="index-middle">
        <!-- Main Content -->
        


<div class="post-container">
    <div class="post-title">
        SVM 支持向量机
    </div>

    <div class="post-meta">
        <span class="attr">发布于：<span>2019-08-03 14:33:37</span></span>
        
        <span class="attr">标签：/
        
        <a class="tag" href="/junlian.github.io/tags/#机器学习" title="机器学习">机器学习</a>
        <span>/</span>
        
        
        </span>
        <span class="attr">访问：<span id="busuanzi_value_page_pv"></span>
</span>
</span>
    </div>
    <div class="post-content no-indent">
        <p><strong>SVM (support vector machines) 是一种分类模型，基本想法是求解能够正确划分训练集，并且几何间隔最大化的分离超平面：</strong></p>
<ul>
<li><strong>模型</strong>：基本模型是定义在特征空间上的间隔最大的线性分类器，可引入核技巧使其成为实质上的非线性分类器。</li>
<li><strong>学习策略</strong>：间隔最大化，学习是在特征空间进行的；</li>
<li><strong>学习算法</strong>：求解凸二次规划的最优化算法。</li>
</ul>
<p><strong>模型由简至繁分三种：</strong><br>-（1） <strong>线性可分支持向量机</strong>：当训练数据线性可分，通过硬间隔最大化学习，又称硬间隔支持向量机；<br>-（2）<strong>线性支持向量机</strong>：当训练数据近似线性可分，通过软间隔最大化学习，又称软间隔支持向量机；<br>-（3）<strong>非线性支持向量机</strong>：当训练数据线性不可分，通过核技巧及软间隔最大化学习。</p>
<p><strong>核技巧</strong><br>核方法是比支持向量机更一般的机器学习方法。</p>
<p>当输入空间为欧氏空间或离散集合、特征空间为<code>希尔伯特空间</code>时，核函数 (kernel function) 表示将输入从输入空间映射到特征空间得到的<strong>特征向量之间的内积</strong>。通过核函数学习非线性支持向量机，相当于隐式地在高维的特征空间中学习线性支持向量机。这样的方法称为核技巧。</p>
<blockquote>
<p><strong>希尔伯特空间：</strong>是欧几里得空间的推广，不再局限于有限维的情形，也是一个内积空间，有距离和角的概念，所以也有正交性和垂直性的概念。</p>
</blockquote>
<h2 id="1-线性可分支持向量机"><a href="#1-线性可分支持向量机" class="headerlink" title="1 线性可分支持向量机"></a>1 线性可分支持向量机</h2><p>假设输入空间和特征空间是两个不同的空间，输入空间是欧式空间或离散空间，特征空间是欧式空间或希尔伯特空间，线性可分支持向量机和线性支持向量机都假设这两个空间的元素一一对应，用一个线性映射将输入空间中的输入为特征空间中的特征向量。非线性支持向量机是用一个非线性映射将输入映射为特征向量。</p>
<h3 id="1-1-定义"><a href="#1-1-定义" class="headerlink" title="1.1 定义"></a>1.1 定义</h3><p>假设训练数据是线性可分的，<strong>学习的目标是在特征空间找到一个超平面，能将实例分到不同的类：</strong></p>
<ul>
<li><strong>超平面：</strong> $w\cdot x+b=0$ ，由法向量 $w$ 和截距 $b$ 决定，可用 $(w,b)$ 表示，对应的是能将两类数据正确划分<strong>且间隔最大的直线</strong>；</li>
<li><strong>分类决策函数：</strong>$f(x)=sign(w\cdot x+b)$ ，称为线性可分支持向量机，超平面将特征空间划为两部分，<strong>法向量指向的一侧是正类，另一侧是负类</strong>。</li>
</ul>
<h3 id="1-2-函数间隔和几何间隔"><a href="#1-2-函数间隔和几何间隔" class="headerlink" title="1.2 函数间隔和几何间隔"></a>1.2 函数间隔和几何间隔</h3><p><strong>1. 函数间隔</strong><br><strong>用函数间隔 $y(w\cdot x+b)$ 来表示分类的正确性和确信度</strong>：分类预测的确信程度用一个点距离分类超平面的远近表示，在超平面 <strong> $w\cdot x+b=0$ 确定的情况下：（1） </strong>点 $x$ 到超平面的距离<strong>：  $|w\cdot x+b|$ ；（2） </strong>分类是否正确**：$w\cdot x+b$ 的符号与类标记 $y$ 的符号是否一致；所以用 $y(w\cdot x+b)$ 来表示分类的正确性和确信度，也就是函数间隔。</p>
<p>对单个样本点 $(x_i, y_i)$ ，函数间隔为 $\hat\gamma_i=y_i(w \cdot x_i + b)$ ，超平面 $(w,b)$ 关于训练数据集 $T$ 的函数间隔为<strong>所有样本点的函数间隔最小值</strong>，即  $\hat\gamma=min_{i=1,…,N}\hat\gamma_i$</p>
<p><strong>2. 几何间隔</strong><br>函数间隔存在的问题是，只要成比例改变 $w$ 和 $b$ ，比如改为  $2w$ 和 $2b$ ，超平面并没有改变，但函数间隔却变为原来的两倍。所以通过<strong>对法向量进行规范化 $||w|| = 1$</strong>使间隔确定，即把函数间隔变为了几何间隔。此时样本 $(x_i, y_i)$ 到超平面的几何距离为带符号的距离：</p>
<script type="math/tex; mode=display">\gamma_i=y_i(\frac{w}{||w||} \cdot x_i + \frac{b}{||w||})</script><p>$||w||$ 为 $w$ 的 $L_2$ 范数，当正确分类时几何距离就是样本到超平面的距离。超平面 $(w,b)$ 关于训练数据集 $T$ 的几何间隔为<strong>所有样本点的几何间隔最小值</strong>，即：</p>
<script type="math/tex; mode=display">\gamma=\mathop{min}\limits_{i=1,...,N}\gamma_i</script><p><strong>3. 函数间隔 $\hat\gamma_i$ 和几何间隔 $\gamma_i$ </strong></p>
<ul>
<li>$\gamma=\frac{\hat\gamma_i}{||w||}$</li>
<li>$w$ 和 $b$ 成比例改变，超平面没改变，几何间隔不变，函数间隔按比例改变。</li>
</ul>
<h3 id="1-3-最大间隔法"><a href="#1-3-最大间隔法" class="headerlink" title="1.3 最大间隔法"></a>1.3 最大间隔法</h3><p>对训练数据集找到几何间隔最大的超平面意味着<strong>以充分大的确信度</strong>对训练数据进行分类。不仅将正负实例分开，<strong>对离超平面最近的最难分的实例点也有足够大的确信度将它们分开</strong>，这样对未知的新实例才可能有很好的分类预测能力。</p>
<p>对线性可分的训练集，分离超平面由无穷多个（等价于感知机），但<strong>几何间隔最大的分离超平面是唯一的</strong>。</p>
<ol>
<li><strong>最大间隔分离超平面</strong><br>求解几何间隔最大的分离超平面，可以表示为下面的约束最优化问题：<script type="math/tex; mode=display">\mathop{max} \limits_{w,b}\ \gamma</script><script type="math/tex; mode=display">s.t. \ y_i(\frac{w}{||w||}\cdot x_i + \frac{b}{||w||}) \ge \gamma, \ \ i=1,2,...,N</script>约束条件表示的是超平面 $(w,b)$ 关于每个训练样本点的几何间隔至少是 $\gamma$</li>
</ol>
<p>结合几何间隔和函数间隔的关系，可以改写为：</p>
<script type="math/tex; mode=display">\mathop{max} \limits_{w,b}\ \frac{\hat\gamma}{||w||}</script><script type="math/tex; mode=display">s.t. \ y_i(w\cdot x_i + b) \ge \hat\gamma, \ \ i=1,2,...,N</script><p>其中函数间隔 $\hat\gamma$ 的取值不影响最优化问题的解（随便将$w$ 和 $b$ 成比例扩大为 $\lambda w$ 和 $\lambda b$ ，函数间隔变为 $\lambda \hat\gamma$，对不等式约束没有影响），所以取 $\hat\gamma=1$ ，同时最大化 $\frac{1}{||w||}$ 和最小化 $\frac{1}{2}||w||^2$ 是等价的，所以线性可分支持向量机学习的最优化问题变为凸二次规划问题，即最大间隔法的约束优化问题。</p>
<p><strong>算法：线性可分支持向量机学习算法——最大间隔法</strong><br>输出：最大间隔分离超平面和分类决策函数。<br>算法：构造并求解约束最优化问题：</p>
<script type="math/tex; mode=display">\mathop{min} \limits_{w,b}\ \frac{1}{2}||w||^2</script><script type="math/tex; mode=display">s.t. \ y_i(w\cdot x_i + b) - 1 \ge 0, \ \ i=1,2,...,N</script><p>求得最优解 $w^{<em>}$，$b^{</em>}$，由此得到分离超平面：</p>
<script type="math/tex; mode=display">w^*\cdot x + b^* = 0</script><p>分类决策函数：</p>
<script type="math/tex; mode=display">f(x) = sign(w^*\cdot x + b^*)</script><ol>
<li><p><strong>最大间隔分离超平面的存在唯一性</strong><br>若训练数据集 $T$ <strong>线性可分</strong>，则可将训练数据集中的样本点完全正确分开的最大间隔分离超平面存在且唯一。</p>
</li>
<li><p><strong>支持向量和间隔边界</strong><br>在线性可分情况下，训练数据集的样本点中与分离超平面<strong>距离最近的样本点</strong>的实例称为支持向量，支持向量是使约束条件等号成立的点：</p>
<script type="math/tex; mode=display">y_i(w\cdot x_i + b) - 1=0</script><p>对 $y_i=+1$ 的正例，支持向量在超平面 $H_1:w \cdot x + b = 1$;<br>对 $y_i=-1$ 的负例，支持向量在超平面 $H_1:w \cdot x + b = -1$;<br><strong>在 $H_1$ 和 $H_2$ 上的点就是支持向量</strong>。 $H_1$ 和 $H_2$ 平行，且没有点落在它们之间，之间形成的长带的宽度称为<strong>间隔 (margin)</strong>，间隔等于 $\frac{2}{||w||}$，$H_1$ 和 $H_2$ 称为<strong>间隔边界</strong>。</p>
</li>
</ol>
<p>在<strong>决定分类超平面时只有支持向量起作用</strong>，其他实例点不起作用，其他点移动或去掉都不会影响，所以支持向量机由很少的“重要的”训练样本确定。</p>
<h3 id="1-4-对偶算法"><a href="#1-4-对偶算法" class="headerlink" title="1.4 对偶算法"></a>1.4 对偶算法</h3><p>为求解最大间隔法中的最优化问题，将它作为原始最优化问题，应用拉格朗日对偶性，通过求解对偶问题得到原始问题的最优解，就是线性可分支持向量机的对偶算法。优点是（1）对偶问题往往更容易求解；（2）<strong>自然引入核函数，方便推广到非线性分类问题</strong>。</p>
<p>首先构建拉格朗日函数，对 $ y_i(w\cdot x_i + b) - 1 \ge 0,   i=1,2,…,N$ 中每一个不等式约束引入拉格朗日乘子 $\alpha_i \ge 0,  i=1, 2,…,N$，定义拉格朗日函数为：</p>
<script type="math/tex; mode=display">L(w,b,a) = \frac{1}{2}||w||^2 - \sum\limits_{i=1}^{N}\alpha_i y_i(w\cdot x_i + b) + \sum\limits_{i=1}^{N}\alpha_i</script><p>其中  $\alpha = (\alpha_1, \alpha_2, …, \alpha_N)^T$ 为拉格朗日乘子向量。根据拉格朗日对偶性，原始问题的对偶问题是极大极小问题：</p>
<script type="math/tex; mode=display">\mathop{max}\limits_{\alpha}\mathop{min}\limits_{w,b}L(w,b,a)</script><p>即需要先求 $L(w,b,a)$ 对 $w, b$ 的极小，再求对 $\alpha$ 的极大。</p>
<p>（1）求 $\mathop{min}\limits_{w,b}L(w,b,a)$<br>将 $L(w,b,\alpha)$ 分别对 $w,b$ 求偏导数并令其等于0：</p>
<script type="math/tex; mode=display">\nabla_wL(w, b, \alpha) = w-\sum\limits_{i=1}^N \alpha_i y_ix_i = 0</script><script type="math/tex; mode=display">\nabla_bL(w, b, \alpha) = -\sum\limits_{i=1}^N \alpha_iy_i = 0</script><p>得：</p>
<script type="math/tex; mode=display">w = \sum\limits_{i=1}^N \alpha_i y_ix_i</script><script type="math/tex; mode=display">\sum\limits_{i=1}^N \alpha_iy_i = 0</script><p>带入拉格朗日函数 $L(w,b,a)$ 中，得：</p>
<script type="math/tex; mode=display">L(w,b,a) = \frac{1}{2}\sum\limits_{i=1}^N \sum\limits_{j=1}^N \alpha_i \alpha_j  y_i y_j (x_i \cdot x_j)  - \sum\limits_{i=1}^{N}\alpha_i y_i((\sum\limits_{i=1}^N \alpha_i y_ix_i) \cdot x_i + b) + \sum\limits_{i=1}^{N}\alpha_i</script><script type="math/tex; mode=display">=  \frac{1}{2}\sum\limits_{i=1}^N \sum\limits_{j=1}^N \alpha_i \alpha_j  y_i y_j (x_i \cdot x_j)  + \sum\limits_{i=1}^{N}\alpha_i</script><p> 即:</p>
<script type="math/tex; mode=display">\mathop{min}\limits_{w,b}L(w,b,a) = \frac{1}{2}\sum\limits_{i=1}^N \sum\limits_{j=1}^N \alpha_i \alpha_j  y_i y_j (x_i \cdot x_j)  + \sum\limits_{i=1}^{N}\alpha_i</script><p>（2）求 $\mathop{min}\limits_{w,b}L(w,b,a)$ 对 $\alpha$ 的极大，即是对偶问题：</p>
<script type="math/tex; mode=display">\mathop{max}\limits_\alpha -\frac{1}{2}\sum\limits_{i=1}^{N}\sum\limits_{j=1}^N \alpha_i \alpha_j y_i y_j (x_i \cdot x_j) + \sum\limits_{i=1}^N \alpha_i</script><script type="math/tex; mode=display">s.t. \sum\limits_{i=1}^N \alpha_i y_i = 0</script><script type="math/tex; mode=display">\alpha_i \ge 0, i = 1, 2, ..., N</script><p>将上式的目标函数由求极大转换为求极小，得到下面与之等价的对偶最优化问题：</p>
<script type="math/tex; mode=display">\mathop{min}\limits_\alpha \frac{1}{2}\sum\limits_{i=1}^{N}\sum\limits_{j=1}^N \alpha_i \alpha_j y_i y_j (x_i \cdot x_j) - \sum\limits_{i=1}^N \alpha_i</script><script type="math/tex; mode=display">s.t. \sum\limits_{i=1}^N \alpha_i y_i = 0</script><script type="math/tex; mode=display">\alpha_i \ge 0, i = 1, 2, ..., N</script><p>对线性可分训练集，假设对偶最优化问题对 $\alpha^{<em>}$ 的解为 $\alpha^{</em>} = (\alpha_1^{<em>}, \alpha_2^{</em>}, …, \alpha_N^{<em>})^T$ ，可以由 $\alpha^{</em>}$  求得原始最优化问题对 $(w,b)$ 的解 $w^{<em>}, b^{</em>}$，即<strong>存在下标 $j$，使得 $\alpha_j ^{<em>} \gt 0$，即选择 $\alpha^{</em>}$ 的一个正分量 $\alpha_j^{*}$</strong>：</p>
<script type="math/tex; mode=display">w^{*} = \sum\limits_{i=1}^{N}\alpha_i^{*}y_ix_i</script><script type="math/tex; mode=display">b^{*} = y_j -  \sum\limits_{i=1}^{N}\alpha_i^{*}y_i(x_i \cdot x_j)</script><p>分类超平面可写为：</p>
<script type="math/tex; mode=display">\sum\limits_{i=1}^{N}\alpha_i^{*}y_i(x_i \cdot x) + b^{*} = 0</script><p>分类决策函数可写为：</p>
<script type="math/tex; mode=display">f(x) = sign(\sum\limits_{i=1}^{N}\alpha_i^{*}y_i(x_i \cdot x) + b^{*})</script><p>也就是说，分类决策函数只依赖于输入 $x$ 和训练样本输入的内积，上式称为线性可分支持向量机的对偶形式。</p>
<p>因为选择 $\alpha^{<em>}$ 的正分量 $\alpha_j^{</em>}$，所以 $w^{<em>}$ 和 $b^{</em>}$ 只依赖于训练数据中对应于  $\alpha_i^{<em>} \gt 0$ 的样本点 $(x_i, y_i)$，其他样本点没有影响，我们将训练集中**对应于   $\alpha_i^{</em>} \gt 0$ 的实例点 $x_u \in \mathbb R^n$ 称为支持向量**。</p>
<h2 id="2-线性支持向量机"><a href="#2-线性支持向量机" class="headerlink" title="2 线性支持向量机"></a>2 线性支持向量机</h2><h3 id="2-1-线性支持向量机"><a href="#2-1-线性支持向量机" class="headerlink" title="2.1 线性支持向量机"></a>2.1 线性支持向量机</h3><p>当训练数据中有一些特异点，<strong>将这些特异点除去后，剩下大部分的样本点组成的集合是线性可分的</strong>，则称训练集不是线性可分的。线性不可分意味着某些样本点不能满足函数间隔大于等于1的约束条件，所以通过对每个样本点<strong>引入一个松弛变量 $\xi_i\ge0$</strong> ，使函数间隔加上松弛变量大于等于1，将硬间隔最大化改为<strong>软间隔最大化</strong>，使线性可分支持向量机学习方法扩展到线性不可分问题上。此时约束条件变为：</p>
<script type="math/tex; mode=display">y_i(w \cdot x_i + b) \gt 1-\xi_i</script><p>同时给每个松弛变量支付一个代价，目标函数由原来的 $\frac{1}{2}||w||^2$ 变为：</p>
<script type="math/tex; mode=display">\frac{1}{2}||w||^2 + C\sum\limits_{i=1}^N \xi_i</script><p>$C$ 为惩罚参数，需要根据具体问题决定，$C$ 越大对误分类的惩罚越大。目标函数包含两层含义：（1）通过使 $\frac{1}{2}||w||^2$ 尽量小，来让间隔尽量大；（2）使误分类点的个数尽量小。二者的系数用$C$来调节。</p>
<p>其他的就可以和训练数据集线性可分时一样来考虑线性不可分时的线性支持向量机学习：<br>（1）构造并求解约束最优化问题：</p>
<script type="math/tex; mode=display">\mathop{min} \limits_{w,b}\ \frac{1}{2}||w||^2 + C\sum\limits_{i=1}^N \xi _i</script><script type="math/tex; mode=display">s.t. \ y_i(w\cdot x_i + b) \ge 1 - \xi_i, \ \ i=1,2,...,N</script><script type="math/tex; mode=display">\xi_i \ge 0, i=1,2,...,N</script><p>求得最优解 $w^{<em>}$，$b^{</em>}$，<strong>和线性可分支持向量机不同的是，$b$的解可能不唯一</strong>，而是存在于一个区间。由此得到分离超平面：</p>
<script type="math/tex; mode=display">w^*\cdot x + b^* = 0</script><p>分类决策函数：</p>
<script type="math/tex; mode=display">f(x) = sign(w^*\cdot x + b^*)</script><h3 id="2-2-学习的对偶算法"><a href="#2-2-学习的对偶算法" class="headerlink" title="2.2 学习的对偶算法"></a>2.2 学习的对偶算法</h3><p>（1）选择惩罚参数 $C \gt 0$，构造并求解凸二次规划问题：</p>
<script type="math/tex; mode=display">\mathop{min}\limits_\alpha \frac{1}{2}\sum\limits_{i=1}^N\sum\limits_{j=1}^N\alpha_i\alpha_jy_iy_j(x_i \cdot x_j) - \sum\limits_{i=1}^N\alpha_i</script><script type="math/tex; mode=display">s.t. \sum\limits_{i=1}^N\alpha_iy_i = 0</script><script type="math/tex; mode=display">0\le\alpha_i\le C. i=1,2,...,N</script><p>求得最优解 $\alpha^{<em>} = (\alpha_1^{</em>}, \alpha_2^{<em>},…,\alpha_N^{</em>})^T$<br>（2）计算 $w^{<em>} = \sum\limits_{i=1}^N\alpha_i^{</em>}y_ix_i$<br>选择 $\alpha^{<em>}$ 的一个分量 $\alpha_j^{</em>}$ 满足$0 \le \alpha_j^{*} \le C$，计算：</p>
<script type="math/tex; mode=display">b^{*} = y_j^{*} - \sum\limits_{i=1}^N y_i \alpha_i^{*}(x_i \cdot x_j)</script><p>因为任意一个满足条件$0 \le \alpha_j^{<em>} \le C$的 $\alpha_j^{</em>}$ 都可以计算得$b$，所以理论上 $b$ 的解不是唯一的。</p>
<p>（3）求得分离超平面：</p>
<script type="math/tex; mode=display">w^{*} \cdot x+b^{*} = 0</script><p>分类决策函数：</p>
<script type="math/tex; mode=display">f(x) = sign(w^{*} \cdot x + b^{*})</script><h3 id="2-3-支持向量"><a href="#2-3-支持向量" class="headerlink" title="2.3 支持向量"></a>2.3 支持向量</h3><p>对偶问题的解  $\alpha^{<em>} = (\alpha_1^{</em>}, \alpha_2^{<em>},…,\alpha_N^{</em>})^T$ 中 $0 \lt\alpha_j^{*}    $ 的样本点 $(x_i, y_i)$ 的实例 $x_i$ 称为支持向量（软间隔支持向量），软间隔的支持向量 $x_i$ 可能在间隔边界上，可能在间隔边界与分离超平面之间，或者在分离超平面误分的一侧 （ 由式 $y_i(w\cdot x_i+b) \ge 1-\xi$ 知）：</p>
<ul>
<li>若 $\alpha_j^{*}\lt C$，则$\xi_i = 0$，支持向量 $x_i$ 恰好落在间隔边界上；</li>
<li>若 $\alpha_j^{*}=C$，$ 0&lt;\xi_i &lt; 1$，支持向量 $x_i$ 在间隔边界和分离超平面之间，分类正确；</li>
<li>若 $\alpha_j^{*}=C$，$ \xi_i = 1$，则 $x_i$ 在分离超平面上；</li>
<li>若 $\alpha_j^{*}=C$，$ \xi_i &gt; 1$，支持向量 $x_i$ 在分离超平面误分一侧；</li>
</ul>
<h3 id="2-4-合页损失函数"><a href="#2-4-合页损失函数" class="headerlink" title="2.4 合页损失函数"></a>2.4 合页损失函数</h3><p>线性支持向量机学习还有另外一种解释，就是最小化以下目标函数：</p>
<script type="math/tex; mode=display">\sum\limits_{i=1}^N[1-y_i(w \cdot x_i + b)]_{+} + \lambda||w||^2</script><p>第二项是系数为$\lambda$的$w$的$L_2$范数，是正则化项；第一项是经验损失或经验风险，函数</p>
<script type="math/tex; mode=display">L(y(w\cdot x+b)) = [1-y(w \cdot x+b)]_{+}</script><p>称为合页损失函数（hinge loss function），下标+表示取正值的函数。也就是说当样本点被正确分类且函数间隔大于1时（$y_i(w\cdot x_i+b) &gt; 1$ ），损失是0，否则损失是$1- y_i(w\cdot x_i+b) $ </p>
<p>所以线性支持向量机原始最优化问题：</p>
<script type="math/tex; mode=display">\mathop{min} \limits_{w,b}\ \frac{1}{2}||w||^2 + C\sum\limits_{i=1}^N \xi _i</script><script type="math/tex; mode=display">s.t. \ y_i(w\cdot x_i + b) \ge 1 - \xi_i, \ \ i=1,2,...,N</script><script type="math/tex; mode=display">\xi_i \ge 0, i=1,2,...,N</script><p>等价于最优化问题：</p>
<script type="math/tex; mode=display">\mathop{min} \limits_{w,b}L(y(w\cdot x+b)) = [1-y(w \cdot x+b)]_{+}+ \lambda||w||^2</script><p><strong>感知机损失函数是$ [-y(w \cdot x+b)]_{+}$ ，样本正确分类是损失为0，否则损失是$ -y(w \cdot x+b)$，相比之下，合页损失函数要求更高，不仅要分类正确，还要确信度足够高时损失才是0。</strong></p>
<h2 id="3-非线性支持向量机与核函数"><a href="#3-非线性支持向量机与核函数" class="headerlink" title="3. 非线性支持向量机与核函数"></a>3. 非线性支持向量机与核函数</h2><h3 id="3-1-核技巧"><a href="#3-1-核技巧" class="headerlink" title="3.1 核技巧"></a>3.1 核技巧</h3><h4 id="3-1-1-非线性分类问题"><a href="#3-1-1-非线性分类问题" class="headerlink" title="3.1.1 非线性分类问题"></a>3.1.1 非线性分类问题</h4><p>只有通过非线性模型才能很好地进行分类的问题，比如分界线是一个椭圆曲线。一般来说，如果能用$\mathbb{R^n}$中的一个超曲面将正负例正确分开，则称问题是非线性可分问题。</p>
<p>非线性问题不好求解，核技巧先进行一个非线性变换，把非线性问题变换为线性问题，通过解变换后的线性问题的方法，求解原来的非线性问题。也就是说求解过程分为两步：（1）用一个变换将原空间的数据映射到新空间；（2）在新空间里<strong>用线性分类学习方法</strong>从训练数据中学习分类模型。</p>
<p>核技巧用在支持向量机中，基本想法就是通过一个非线性变换将输入空间（欧式空间或离散集合）对应于一个特征空间（希尔伯特空间$\mathbb{H}$），使得在输入空间中的超曲面模型对应于特征空间中的超平面模型（支持向量机），这样通过在特征空间中求解线性支持向量机就可以学习分类任务。</p>
<h4 id="3-1-2-核函数"><a href="#3-1-2-核函数" class="headerlink" title="3.1.2 核函数"></a>3.1.2 核函数</h4><p>核函数想法是，<strong>在学习和预测中不显式地定义映射函数$\phi(x)$，只定义核函数 $K(x,z)$</strong> 。</p>
<p>$\mathcal{X}$ 是输入空间（欧式空间或离散集合），$\mathcal{H}$为特征空间（希尔伯特空间），存在一个从$\mathcal{X}$ 到 $\mathcal{H}$的映射：</p>
<script type="math/tex; mode=display">\phi(x)=\mathcal{X} \rightarrow \mathcal{H}</script><p>使得对所有 $x, z \in \mathcal{X}$，函数 $K(x,z)$ 满足条件：</p>
<script type="math/tex; mode=display">K(x,z)=\phi(x) \cdot \phi(z)</script><p>则称 $K(x,z)$ 为核函数，$\phi(x)$ 为映射函数，$\phi(x) \cdot \phi(z)$是内积。$\mathcal{H}$一般是高维的，甚至是无穷维的，对给定的核函数 $K(x,z)$ ，特征空间$\mathcal{H}$和映射函数 $\phi(x)$ 取法并不唯一。</p>
<h4 id="3-1-3-核技巧在支持向量机中应用"><a href="#3-1-3-核技巧在支持向量机中应用" class="headerlink" title="3.1.3 核技巧在支持向量机中应用"></a>3.1.3 核技巧在支持向量机中应用</h4><p>在线性支持向量机的对偶问题中，无论目标函数还是决策函数，都只涉及输入实例和实例之间的内积，可以把：</p>
<script type="math/tex; mode=display">W(\alpha) = \frac{1}{2}\sum\limits_{i=1}^N \sum\limits_{j=1}^N \alpha_i \alpha_j  y_i y_j (x_i \cdot x_j)  + \sum\limits_{i=1}^{N}\alpha_i</script><p> 中的内积 $x_i \cdot x_j$ 换成核函数 $K(x_i, x_j) = \phi(x_i) \cdot \phi(x_j)$表示：</p>
<script type="math/tex; mode=display">W(\alpha) = \frac{1}{2}\sum\limits_{i=1}^N \sum\limits_{j=1}^N \alpha_i \alpha_j  y_i y_j K(x_i, x_j) + \sum\limits_{i=1}^{N}\alpha_i</script><p>同样分类决策函数也可以表示为：</p>
<script type="math/tex; mode=display">f(x) = sign(\sum\limits_{i=1}^{N}\alpha_i^{*}y_iK(x_i, x)+ b^{*})</script><script type="math/tex; mode=display">= sign(\sum\limits_{i=1}^{N}\alpha_i^{*}y_i \phi (x_i) \cdot \phi(x) + b^{*})</script><p>等价于经过映射函数 $\phi$ 将原来的输入空间变换到新的特征空间，当映射函数是非线性函数时，学习到的含有核函数的支持向量机就是非线性分类模型。</p>
<p>也就是说，<strong>核技巧就是利用线性分类学习方法和核函数解决非线性问题的技术</strong>：</p>
<ul>
<li><strong>在给定核函数 $K(x_i, x_j)$的条件下，可以利用解线性分类问题的方法求解非线性分类问题的支持向量机；</strong></li>
<li><strong>学习是隐式地在特征空间进行的，不需要显式地定义特征空间和映射函数；</strong></li>
<li><strong>核函数一般依赖领域知识直接选择。</strong></li>
</ul>
<h3 id="3-2-正定核"><a href="#3-2-正定核" class="headerlink" title="3.2 正定核"></a>3.2 正定核</h3><p>通常所说的核函数就是正定核函数 (positive definite kernel function)， $K(x_i, x_j)$ 要成为核函数的充要条件：设$\mathcal K:\mathcal X \times \mathcal X \rightarrow \mathcal R$ 是对称函数，则 $\mathcal K(x,z)$ 为正定核函数的<strong>充要条件是对任意 $x_i \in \mathcal X, =1,2,…,m$， $\mathcal K(x,z)$ 对应的Gram矩阵</strong>：</p>
<script type="math/tex; mode=display">K=[K(x_i, x_j)]_{m\times m}</script><p><strong>是半正定矩阵。</strong></p>
<p><strong>正定核的等价定义</strong>：满足以上条件的 $\mathcal K(x,z)$ ，就是正定核。</p>
<p><strong>实际中往往应用已经有的核函数</strong>，因为对一个具体函数来说检验他是否为正定核函数很不容易因为要对任意有限输入集验证对应的Gram矩阵是否正定。</p>
<h3 id="3-3-常用核函数"><a href="#3-3-常用核函数" class="headerlink" title="3.3 常用核函数"></a>3.3 常用核函数</h3><ol>
<li><p>多项式核函数（polynomial kernel function）</p>
<script type="math/tex; mode=display">\mathcal K(x,z)=(x \cdot z + 1)^p</script><p>对应的支持向量机是一个 $p$ 次的多项式分类器，此时分类决策函数是：</p>
<script type="math/tex; mode=display">f(x) = sign(\sum\limits_{i=1}^{N_s}\alpha_i^{*}y_i (x_i \cdot x + 1)^p  + b^{*})</script></li>
<li><p>高斯核函数 (Gaussian kernel function)</p>
<script type="math/tex; mode=display">\mathcal K(x,z)=exp(-\frac{||x-z||^2}{2\sigma^2})</script><p>对应的支持向量机是高斯径向基函数 (radial basis function) 器，此时分类决策函数是：</p>
<script type="math/tex; mode=display">f(x) = sign(\sum\limits_{i=1}^{N_s}\alpha_i^{*}y_i exp(-\frac{||x-x_i||^2}{2\sigma^2})  + b^{*})</script></li>
<li><p>字符串核函数（string kernel function）<br>核函数还可以定义在离散数据的集合上，字符串核函数就是定义在字符串集合上的核函数，在文本分类、信息检索、生物信息学等方面都有应用。</p>
</li>
</ol>
<h3 id="3-4-非线性支持向量机"><a href="#3-4-非线性支持向量机" class="headerlink" title="3.4 非线性支持向量机"></a>3.4 非线性支持向量机</h3><p>只需要将线性支持向量机对偶形式中的内积改为核函数。非线性支持向量机学习算法：<br>（1）选取适当的核函数 $K(x, z)$ 和适当的参数 $C$ ，构造并求解最优化问题：</p>
<script type="math/tex; mode=display">\mathop{min}\limits_\alpha \frac{1}{2}\sum\limits_{i=1}^N\sum\limits_{j=1}^N\alpha_i\alpha_jy_iy_jK(x_i, x_j) - \sum\limits_{i=1}^N\alpha_i</script><script type="math/tex; mode=display">s.t. \sum\limits_{i=1}^N\alpha_iy_i = 0</script><script type="math/tex; mode=display">0\le\alpha_i\le C. i=1,2,...,N</script><p>求得最优解 $\alpha^{<em>} = (\alpha_1^{</em>}, \alpha_2^{<em>},…,\alpha_N^{</em>})^T$<br>（2）计算 $w^{<em>} = \sum\limits_{i=1}^N\alpha_i^{</em>}y_ix_i$<br>选择 $\alpha^{<em>}$ 的一个分量 $\alpha_j^{</em>}$ 满足$0 \le \alpha_j^{*} \le C$，计算：</p>
<script type="math/tex; mode=display">b^{*} = y_j^{*} - \sum\limits_{i=1}^N y_i \alpha_i^{*}K(x_i, x_j)</script><p>因为任意一个满足条件$0 \le \alpha_j^{<em>} \le C$的 $\alpha_j^{</em>}$ 都可以计算得$b$，所以理论上 $b$ 的解不是唯一的。</p>
<p>（3）求得分离超平面：</p>
<script type="math/tex; mode=display">w^{*} \cdot x+b^{*} = 0</script><p>分类决策函数：</p>
<script type="math/tex; mode=display">f(x) = sign(\sum\limits_{i=1}^{N_s}\alpha_i^{*}y_i K(x, x_i)  + b^{*})</script><h2 id="4-序列最小最优化算法"><a href="#4-序列最小最优化算法" class="headerlink" title="4 序列最小最优化算法"></a>4 序列最小最优化算法</h2><p>支持向量机的学习问题可形式化为求解凸二次规划问题，当训练样本容量很大时，如何高效地实现支持向量机就成了问题。序列最小最优化 (sequential minimal optimization, SMO) 算法就是用于实现支持向量机学习。</p>
<p>SMO算法是一种<strong>启发式算法</strong>，基本思路是，如果所有变量的解都满足此最优化问题的 KKT 条件，那么这个最优化问题的解酒得到了。因为KKT条件是该优化问题的充分必要条件。</p>
<p>SMO算法将原问题不断分解为子问题并对子问题求解，进而达到求解原问题的目的。子问题的两个变量中只有一个是自由变量，</p>
<h3 id="SMO-算法"><a href="#SMO-算法" class="headerlink" title="SMO 算法"></a>SMO 算法</h3><p>输入：训练集，精度 $\epsilon$<br>输出：近似解 $\hat \alpha$<br>（1）取初值 $\alpha^{(0)} = 0$，令 $k=0$;<br>（2）选取优化变量$\alpha_1^{(k)}, \alpha_2^{(k)}$，解析求解两个变量的最优化问题：</p>
<p>求得最优解$\alpha_1^{(k+1)}, \alpha_2^{(k+1)}$，更新$\alpha$ 为 $\alpha^{k+1}$<br>（3）若在精度 $\epsilon$ 范围内满足停机条件则转（4），否则令 $k=k+1$，转（2）；<br>（4）取$\hat \alpha = \alpha ^{k+1}$</p>

        
        <br />
        <div id="comment-container">
        </div>
        <div id="disqus_thread"></div>

        <div id="lv-container">
        </div>

    </div>
</div>

    </div>
</div>


<footer class="footer">
    <ul class="list-inline text-center">
        
        

        

        

        

        

    </ul>
    
    <p>
        <span id="busuanzi_container_site_pv">
            <span id="busuanzi_value_site_pv"></span>PV
        </span>
        <span id="busuanzi_container_site_uv">
            <span id="busuanzi_value_site_uv"></span>UV
        </span>
        Created By <a href="https://hexo.io/">Hexo</a>  Theme <a href="https://github.com/aircloud/hexo-theme-aircloud">AirCloud</a></p>
</footer>




<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });

</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<!-- <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> -->

</body>

<script>
    // We expose some of the variables needed by the front end
    window.hexo_search_path = "search.json"
    window.hexo_root = "/junlian.github.io/"
    window.isPost = true
</script>
<script src="https://cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script>
<script src="/junlian.github.io/js/index.js"></script>
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




</html>
