<!DOCTYPE html>
<html lang="en">
<head><meta name="generator" content="Hexo 3.9.0">
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="google-site-verification" content="xBT4GhYoi5qRD5tr338pgPM5OWHHIDR6mNg1a3euekI">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Juliet&#39;s blog">
    <meta name="keyword" content>
    <link rel="shortcut icon" href="/junlian.github.io/img/favicon.ico">

    <title>
        
        Introduction to Big Data - 学 | 慢慢来
        
    </title>

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/junlian.github.io/css/aircloud.css">
    <link rel="stylesheet" href="/junlian.github.io/css/gitment.css">
    <!--<link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css">-->
    <link href="//at.alicdn.com/t/font_620856_pl6z7sid89qkt9.css" rel="stylesheet" type="text/css">
    <!-- ga & ba script hoook -->
    <script></script>
</head>

<body>

<div class="site-nav-toggle" id="site-nav-toggle">
    <button>
        <span class="btn-bar"></span>
        <span class="btn-bar"></span>
        <span class="btn-bar"></span>
    </button>
</div>

<div class="index-about">
    <i> 所谓诚其意者，毋自欺也，如恶恶臭，如好好色，此之谓自谦，故君子必慎其独也。 ——摘《大学》 </i>
</div>

<div class="index-container">
    
    <div class="index-left">
        
<div class="nav" id="nav">
    <div class="avatar-name">
        <div class="avatar ">
            <img src="/junlian.github.io/img/avatar.jpg" />
        </div>
        <div class="name">
            <i>慎思之，笃行之</i>
        </div>
    </div>
    <div class="contents" id="nav-content">
        <ul>
            <li >
                <a href="/junlian.github.io/">
                    <i class="iconfont icon-shouye1"></i>
                    <span>主页</span>
                </a>
            </li>
            <li >
                <a href="/junlian.github.io/tags">
                    <i class="iconfont icon-biaoqian1"></i>
                    <span>标签</span>
                </a>
            </li>
            <li >
                <a href="/junlian.github.io/archives">
                    <i class="iconfont icon-guidang2"></i>
                    <span>存档</span>
                </a>
            </li>
            <li >
                <a href="/junlian.github.io/about/">
                    <i class="iconfont icon-guanyu2"></i>
                    <span>关于</span>
                </a>
            </li>
            
            <li>
                <a id="search">
                    <i class="iconfont icon-sousuo1"></i>
                    <span>搜索</span>
                </a>
            </li>
            
        </ul>
    </div>
    
        <div id="toc" class="toc-article">
    <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Why-and-where"><span class="toc-text">Why and where</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#What-launched-the-big-data-era"><span class="toc-text">What launched the big data era?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Applications"><span class="toc-text">Applications</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Wildfire-Analytics"><span class="toc-text">Wildfire Analytics</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Precision-medicine"><span class="toc-text">Precision medicine</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Data-sources"><span class="toc-text">Data sources</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Generated-by-Machines"><span class="toc-text">Generated by Machines</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Generated-by-people-the-unstructured-challenge"><span class="toc-text">Generated by people: the unstructured challenge</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Generated-by-Organizations"><span class="toc-text">Generated by Organizations</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Characteristics-of-Big-Data"><span class="toc-text">Characteristics of Big Data</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#SIx-V’s"><span class="toc-text">SIx V’s</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Volume"><span class="toc-text">Volume</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Variety"><span class="toc-text">Variety</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Velocity"><span class="toc-text">Velocity</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Veracity"><span class="toc-text">Veracity</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Define-data-quality"><span class="toc-text">Define data quality</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#The-reasons-for-uncertain"><span class="toc-text">The reasons for uncertain</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Problem"><span class="toc-text">Problem</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Challenges-on-data-quality"><span class="toc-text">Challenges on data quality</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Valence"><span class="toc-text">Valence</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Feature"><span class="toc-text">Feature</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Measures"><span class="toc-text">Measures</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Chellenge"><span class="toc-text">Chellenge</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Innovative-technologies"><span class="toc-text">Innovative technologies</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Data-science"><span class="toc-text">Data science</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Data-science-1"><span class="toc-text">Data science</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Big-Data-Strategy"><span class="toc-text">Big Data Strategy</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Strategy"><span class="toc-text">Strategy</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Big-data-Strategy"><span class="toc-text">Big data Strategy</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Five-P’s-of-data-science"><span class="toc-text">Five P’s of data science</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Product"><span class="toc-text">Product</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#People"><span class="toc-text">People</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Purpose"><span class="toc-text">Purpose</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Process"><span class="toc-text">Process</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Platforms"><span class="toc-text">Platforms</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Programmability"><span class="toc-text">Programmability</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Accountability"><span class="toc-text">Accountability</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Asking-the-right-questions"><span class="toc-text">Asking the right questions</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Data-Science-Process"><span class="toc-text">Data Science Process</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Acquiring-data"><span class="toc-text">Acquiring data</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Prepare-1-Exploring-Data"><span class="toc-text">Prepare 1: Exploring Data</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Prepare-2-Pre-Processing-Data"><span class="toc-text">Prepare 2: Pre-Processing Data</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Analyzing-Data"><span class="toc-text">Analyzing Data</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Communicating-Results"><span class="toc-text">Communicating Results</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Turning-Insights-into-Action"><span class="toc-text">Turning Insights into Action</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Basic-Scalable-Computing-Concepts"><span class="toc-text">Basic Scalable Computing Concepts</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Distributed-File-System"><span class="toc-text">Distributed File System</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Scalable-computing"><span class="toc-text">Scalable computing</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Parallel-computers"><span class="toc-text">Parallel computers</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Commodity-cluster"><span class="toc-text">Commodity cluster</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Programming-Models"><span class="toc-text">Programming Models</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#The-Requirements"><span class="toc-text">The Requirements</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#MapReduce"><span class="toc-text">MapReduce</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Hadoop"><span class="toc-text">Hadoop</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Overarching-themes-and-goals"><span class="toc-text">Overarching themes and goals</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#The-Hadoop-Ecosystem"><span class="toc-text">The Hadoop Ecosystem</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#HDFS"><span class="toc-text">HDFS</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Two-capabilities"><span class="toc-text">Two capabilities</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Two-key-components"><span class="toc-text">Two key components</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#YARN"><span class="toc-text">YARN</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Share-Hadoop-across-applications"><span class="toc-text">Share Hadoop across applications</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Architecture"><span class="toc-text">Architecture</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MapReduce-1"><span class="toc-text">MapReduce</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Map-and-Reduce"><span class="toc-text">Map and Reduce</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#WordCount"><span class="toc-text">WordCount</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#The-points-of-data-parallelism"><span class="toc-text">The points of data parallelism</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#The-tasks-that-MapReduce-is-BAD-for"><span class="toc-text">The tasks that MapReduce is BAD for</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Reconsider-Hadoop"><span class="toc-text">Reconsider Hadoop</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#The-key-features-that-make-a-problem-Hadoop-friendly"><span class="toc-text">The key features that make a problem Hadoop friendly</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#The-Hadoop-framework-is-not-the-best-for"><span class="toc-text">The Hadoop framework is not the best for:</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Some-of-the-moving-targets-in-the-Hadoop-ecosystem"><span class="toc-text">Some of the moving targets in the Hadoop ecosystem</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Cloud-Computing"><span class="toc-text">Cloud Computing</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#In-house-capabilities"><span class="toc-text">In-house capabilities</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Cloud-computing-service"><span class="toc-text">Cloud computing service</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Three-main-class-service-models"><span class="toc-text">Three main class service models</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Pre-built-Hadoop-images"><span class="toc-text">Pre-built Hadoop images</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#virtualization-software"><span class="toc-text">virtualization software</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Companies"><span class="toc-text">Companies</span></a></li></ol></li></ol></li></ol>
</div>
    
</div>


<div class="search-field" id="search-field">
    <div class="search-container">
        <div class="search-input">
            <span id="esc-search"> <i class="icon-fanhui iconfont"></i></span>
            <input id="search-input"/>
            <span id="begin-search">搜索</span>
        </div>
        <div class="search-result-container" id="search-result-container">

        </div>
    </div>
</div>

        <div class="index-about-mobile">
            <i> 所谓诚其意者，毋自欺也，如恶恶臭，如好好色，此之谓自谦，故君子必慎其独也。 ——摘《大学》 </i>
        </div>
    </div>
    
    <div class="index-middle">
        <!-- Main Content -->
        


<div class="post-container">
    <div class="post-title">
        Introduction to Big Data
    </div>

    <div class="post-meta">
        <span class="attr">发布于：<span>2019-07-28 11:16:46</span></span>
        
        <span class="attr">标签：/
        
        <a class="tag" href="/junlian.github.io/tags/#BigData" title="BigData">BigData</a>
        <span>/</span>
        
        
        </span>
        <span class="attr">访问：<span id="busuanzi_value_page_pv"></span>
</span>
</span>
    </div>
    <div class="post-content no-indent">
        <h2 id="Why-and-where"><a href="#Why-and-where" class="headerlink" title="Why and where"></a>Why and where</h2><h3 id="What-launched-the-big-data-era"><a href="#What-launched-the-big-data-era" class="headerlink" title="What launched the big data era?"></a>What launched the big data era?</h3><p>a new <strong>torrent of big data</strong> combined with <strong>computing capability</strong> anytime, anywhere has been at the core of the launch of the big data era. </p>
<h3 id="Applications"><a href="#Applications" class="headerlink" title="Applications"></a>Applications</h3><blockquote>
<p><strong>Retail</strong></p>
<ul>
<li>Customer ralationship management</li>
<li>Fraud detection and prevetion</li>
<li>Supply chain optimization</li>
<li>Store location and layout</li>
<li>Dynamic pricing</li>
</ul>
<p><strong>Financial services</strong></p>
<ul>
<li>Algorithmic trading</li>
<li>Fraud detection</li>
<li>Risk analysis</li>
<li>portfolio analysis证券投资组合</li>
</ul>
<p><strong>Advertising and public relations</strong></p>
<ul>
<li>Demand signaling 需求信号</li>
<li>Targeted advertising</li>
<li>Sentiment analysis</li>
<li>Customer acquisition 用户满意度</li>
</ul>
<p><strong>Government</strong></p>
<ul>
<li>Market governance</li>
<li>Weapon system and counter terrorism 反恐</li>
<li>Econometrics</li>
<li>Health informatics 健康信息</li>
</ul>
<p><strong>Manufacturing</strong></p>
<ul>
<li>Product research</li>
<li>Process and quality analysis 过程控制质量优化</li>
<li>Engineering analytics</li>
<li>Distribution optimization 优化调度</li>
<li>Predictive maintenance 预测性维护</li>
</ul>
<p><strong>Media and telecommunications</strong></p>
<ul>
<li>Network optimization</li>
<li>churn prevention 流失预测</li>
<li>Customer scoring 用户评级</li>
<li>Fraud prevention</li>
</ul>
<p><strong>Energy</strong></p>
<ul>
<li>Smart grid</li>
<li>Operational modeling</li>
<li>Exploration</li>
<li>Power-line sensors</li>
</ul>
<p><strong>Healthcare and life sciences</strong></p>
<ul>
<li>Pharmacogenomics[fɑ：məkədʒe’nɒmɪks] 药物基因组学</li>
<li>Pharmaceutical research[ˌfɑ：məsu：tɪkl] 药物研究</li>
<li>Bioinformatics 生物信息学</li>
<li>Clinical outcomes research 临床<h4 id="Wildfire-Analytics"><a href="#Wildfire-Analytics" class="headerlink" title="Wildfire Analytics"></a>Wildfire Analytics</h4>Wildfires can become so severe that we actually call them firestorms.Although we cannot control such fire storms,something we can do is to get ahead of them by predicting their behavior.The management relies heavily on their <strong>direction</strong> and <strong>rate of spread</strong>,we use Big Data to <strong>monitor,predict and manage</strong> a firestorm. </li>
</ul>
<ol>
<li>break up into two components:prediction and response</li>
<li>Integration of diverse streams:</li>
</ol>
<ul>
<li>public on social media sites such as Twitter,people through <strong>devices they carry</strong>.</li>
<li><strong>Sensor data</strong> streaming in from weather stations and satellites,include <strong>temperature,humidity,air pressure</strong>.</li>
<li>image data streaming from mountaintop cameras and satellites.</li>
<li>Organizational data from institutions,including <strong>area maps</strong>,better <strong>service updates</strong> and <strong>field content databases</strong>,which archive how much registers <strong>vegetation</strong> and <strong>other types of fuel</strong> are in the way of a potential fire.(Static,updated at a slow rate,well-curated and verified)</li>
</ul>
</blockquote>
<h4 id="Precision-medicine"><a href="#Precision-medicine" class="headerlink" title="Precision medicine"></a>Precision medicine</h4><p>It is an emerging area of medicine targeted toward an <strong>individual person</strong>. Analysing her <code>genetics</code>, her <code>environment</code>,her <code>daily activities</code> so that one can detect or <strong>predict a health problem</strong> early.prevent disease,<strong>provide the right drug</strong>. Utilize an individual’s genetic profile for his or her own diagnoses and treatment.</p>
<blockquote>
<p><code>For any technology to succeed</code> in real life we need not only a certain level of <strong>maturity of the technology</strong> itself,but a number of enabling factors including <strong>social economic environment,market demands,consumer readiness,cost effectiveness</strong>,all of which must work together.</p>
</blockquote>
<p><strong>The integration of multiple types of data sources.</strong> </p>
<ul>
<li><strong>Sensor data</strong>: <code>digital hospital equipment</code>; <code>fitness devices</code>, including wristbands, watches, shoes and vests, tracking several activity variables like blood pressure, different types off activities, blood glucose levels, oxygen saturation levels. This data is about what happens in your normal life;  <code>mobile healths apps</code>,generated from sensors and people.</li>
<li><strong>electronic health records</strong></li>
<li><strong>genomic profile</strong></li>
</ul>
<blockquote>
<p>25 interesting facts about Big Data:(published on Sep 24,2014)</p>
<ol>
<li>Every 2 days we create as much information as we did from the beginning of time until 2003.</li>
<li>Over 90% of all the data in the world was created in the past 2 years.</li>
<li>The total amount of data being captured and stored by industry doubles every 1.2 years.</li>
<li>Google alone processes on average over 40 thousand search queries per second, making it over 3.5 billion in a single day.</li>
<li>Around 100 hours off video are uploaded to YouTube every minute and it would take you around 15 years to watch every video uploaded by users in one day.</li>
<li>If you burned all of the data created in just one day onto DVDs, you could stack them on top of each other and reach the moon twice.</li>
<li>AT&amp;T is thought to hold the world’s largest volume of data in one unique database - its phone records database is 312 terabytes in size, and contains almost 2 trillion(万亿) rows.</li>
<li>570 new websites spring into existence every minute of every day.</li>
<li>1.9 million IT jobs will be created in the US by 2015 to carry out big data projects.Each of those will be supported by 3 new jobs created outside of IT, meaning a total of 6 million new jobs thanks to big data.</li>
<li>Big data has been used to predict crimes before they happen - a “predictive policing” trial in California was able to identify areas where crime will occur three times more accurately than existing methods of forecasting.</li>
<li>By better integrating big data analytics into healthcare, the industry could save \$300bn a year that’s the equivalent of reducing the healthcare costs of every man, woman and child by \$1,000 a year.</li>
<li>Retailers could increase their profit margins by more than 60% through the full exploitation of big data analytics.</li>
</ol>
</blockquote>
<h3 id="Data-sources"><a href="#Data-sources" class="headerlink" title="Data sources"></a>Data sources</h3><p><strong>Three major sources of Big Data</strong></p>
<ul>
<li>Machines: real time sensors in industrial machinery, environmental sensors, personal health trackers…</li>
<li>People: social media data, status updates…</li>
<li>Organizations: transaction information in databases.</li>
</ul>
<h4 id="Generated-by-Machines"><a href="#Generated-by-Machines" class="headerlink" title="Generated by Machines"></a>Generated by Machines</h4><p><strong>Three main properties of smart devices</strong></p>
<ul>
<li>Connect to other devices or networks</li>
<li>Collect and analyze data autonomously</li>
<li>Provide environmental context</li>
</ul>
<p><strong>in-situ</strong><br>Bringing the computation to where data is located, or generated. Enable <code>real-time actions</code>.</p>
<p><strong>SCADA</strong><br>Supervisory Control and Data Acquisition, a type of industrial control system.</p>
<ul>
<li>monitoring and control</li>
<li>define actions for reduced waste and improved efficiency.</li>
</ul>
<p>Industrial processes</p>
<ul>
<li>manufacturing and power generation</li>
<li>public or private infrastructure processes, including water treatment, oil, and gas pipelines, and electrical power transmission</li>
<li>facility processes, including buildings, airports, ships, and space stations.</li>
</ul>
<h4 id="Generated-by-people-the-unstructured-challenge"><a href="#Generated-by-people-the-unstructured-challenge" class="headerlink" title="Generated by people: the unstructured challenge"></a>Generated by people: the unstructured challenge</h4><p>including texts, images, videos, audio, <strong>internet searches</strong>, emails.</p>
<p><strong>Unstructrued: </strong><br>non-conforming to a predefined data model.<br>including multiple data formats that were mainly built for human consumption</p>
<p>confirmation of unstructured data is time consuming and costly. including acquiring, storing, cleaning ,retrieving, and processing unstructured data can add up to quite and investment before we can start reaping value from this process. </p>
<blockquote>
<p>A challenge is a perfect opportunity.</p>
</blockquote>
<p><strong>technologies</strong><br>Tools are designed from scratch to manage unstructured information and analyze it. </p>
<ul>
<li>Hadoop: an open source big data framework, A majority of these tools are based on it.</li>
<li>Storm and Spark: open source frameworks that handle real time data generated at a fast rate. They can integrate data with any database or data storage technology.</li>
</ul>
<p><strong>NoSQL database</strong><br> <strong>ETL:</strong>  Extract, Transform, Load. It is fairly static and does not fit well with dynamic big data world.</p>
<p>NoSQL Data are based on non-relational concepts and provide data storage options typically on computing clouds.The main advantage is their ability to organize the data for <code>scalable</code> access to <code>fit the problem</code> and objectives pertaining to how the data will be used.</p>
<ol>
<li><strong>Neo4j</strong>: graph database, used to find connections between data sets.</li>
<li><strong>Cassandra:</strong> key value database</li>
</ol>
<p><strong>Application</strong></p>
<ol>
<li>Sentiment analysis</li>
<li>customer behavior modeling and prediction.</li>
<li>disaster management, in the form of societal impact and social selfare. Like Crisis Mappers Net.</li>
</ol>
<h4 id="Generated-by-Organizations"><a href="#Generated-by-Organizations" class="headerlink" title="Generated by Organizations"></a>Generated by Organizations</h4><p><strong>structured but often siloed</strong><br> <strong>data silos: </strong>A giant centralized database to house all the data production within an organization.</p>
<ol>
<li><strong>Produce data</strong></li>
</ol>
<ul>
<li><p><strong>unique to the organization</strong>, each organization has distinct operation practices and business models, which result in a variety of data generation platforms.</p>
</li>
<li><p>common types: commercial transactions, credit cards, government institutions, e-commerce, banking or stock records, medical records, sensors, transactions, clicks and so on.</p>
</li>
</ul>
<ol>
<li><strong>Data model</strong></li>
</ol>
<ul>
<li>Defining each of these <strong>columns</strong> and <strong>fields</strong> in the table.</li>
<li>Defining <strong>relationships</strong> between columns.</li>
</ul>
<ol>
<li><p><strong>SQL</strong><br>the Structured Query Language<br><strong>Querying the data</strong>: to extract data of interest from such tables.</p>
</li>
<li><p><strong>Cloud-based solution</strong><br>Many organizations have traditionally captured data at the <strong>department level</strong>, no one system has access to all data that the organization owns.</p>
</li>
</ol>
<p><strong>Applications</strong><br>Companies that use analytics are: </p>
<ul>
<li>Twice as likely to be in the top quartile of <strong>financial performance</strong> within their industries.</li>
<li>Five times as likely to <strong>make decisions</strong> much faster than market peers.</li>
<li>Three times as likely to execute decisions as intended.</li>
<li>Twice as likely to use data very frequently when making decisions.</li>
</ul>
<p>Integrating big data practices into culture</p>
<ul>
<li>Package delivery: UPS, utilizing complex optimization over large datasets  leads to <strong>route optimizations</strong>. If they can reduce distance traveled by each truck by even one mile, UPS can save a whopping $50 million U.S per year.</li>
<li>Retail: Walmart; Data: Twitter tweets, <strong>local events, local weather</strong>, in-store purchases, online clicks and many other sales, customer and product related data; Find pattern: which products are frequently purchased together, what is the best new product to introduce in their stores, predict demand at the particular location and to customize customer recommendations.</li>
</ul>
<p><strong>Integrating diverse data</strong></p>
<ul>
<li><p>Data integration means bringing together data from diverse sources and turning them into <strong>coherent</strong> and more useful information, we also call this <strong>knowledge</strong>.</p>
</li>
<li><p>Data integration process: discovering, accessing, monitoring data, modeling and transforming data from a variety of sources.</p>
</li>
<li><p>Why need integrate data</p>
</li>
</ul>
<ol>
<li>Make the final product <strong>richer in the number of features</strong>. By integrating environmental sensor and camera data with geographical information system data, tell what the exact location of the fire is in the photo automatically. </li>
<li>Making each data set more <strong>accessible</strong>: By bringing the data together, and providing programmable access to it.</li>
<li>Reduce the overall data <strong>complexity in data-driven</strong> product.</li>
<li><strong>Increase the collaboration</strong> between different parts of your data systems. Each part can clearly see how their data is integrated into the overall system. Including the <strong>user scenarios</strong> and the <strong>security</strong> and <strong>privacy</strong> processes around it.</li>
</ol>
<hr>
<h2 id="Characteristics-of-Big-Data"><a href="#Characteristics-of-Big-Data" class="headerlink" title="Characteristics of Big Data"></a>Characteristics of Big Data</h2><h3 id="SIx-V’s"><a href="#SIx-V’s" class="headerlink" title="SIx V’s"></a>SIx V’s</h3><ol>
<li>Volume: Size, It refers to the vast amounts of data that is generated every second, minutes, hour,  and day in our digitized world.</li>
<li>Variety: Complexity, the ever increasing different forms that data can come in such as text, images, voice, and geospatial data.</li>
<li>Velocity: [vəˈlɒsəti] Speed, the increasing speed at which data is being generated and the pace at which data moves from one point to the next</li>
<li>Veracity: Quality, the biases, noise, and abnormality in data. Or the unmeasurable uncertainties and truthfulness and trustworthiness of data.可信度</li>
<li>Valence: Connectedness</li>
<li>Value: how big data benefit you and your organization.</li>
</ol>
<h3 id="Volume"><a href="#Volume" class="headerlink" title="Volume"></a>Volume</h3><p>The dimension that relates to the sheer size of big data. We call data at an astronomical scale(天文级), pointing to an exponential growth in data volume and storage.</p>
<blockquote>
<p>Define size: </p>
<ul>
<li>100 MBs will hold a couple of encyclopedias百科全书</li>
<li>A DVD is around 5GBs</li>
<li>1 TB would hold around 300 hours of good quality video.</li>
<li>But petabytes are becoming more common to our daily lives.</li>
<li>A ZB is 1 trillion GBs, 10 to the power of 21 $10^{21}$</li>
<li>The next era will be YBs, $10^{24}$; BBs $10^{27}$</li>
</ul>
<p>The power of TEN<br>$10^{12}$ is China<br>$10^{16}$ is a light year.</p>
</blockquote>
<p>businesses and organizations: Improve their end products, whether it is safety, reliability, healthcare, or governance.</p>
<p><strong>Challenges</strong><br>Cost, scalability, and performance related to their storage, access, and processing.</p>
<ol>
<li>Storage: </li>
</ol>
<ul>
<li>Store efficiently</li>
<li>Retrieve fast enough, and move it to processing units.</li>
<li>Networking, bandwidth, cost of storing data. In-house versus cloud storage(内部存储VS云存储)</li>
</ul>
<ol>
<li>Processing: </li>
</ol>
<ul>
<li>Performance and cost of analytical methods, in terms of memory, processing, or IO needs.  </li>
<li>Holistic [həʊˈlɪstɪk] 整体 strategy to handle processing.</li>
<li>Evaluating the options across the dimensions.</li>
</ul>
<h3 id="Variety"><a href="#Variety" class="headerlink" title="Variety"></a>Variety</h3><blockquote>
<p>A form of scalability, not the largeness, but the increased diversity.</p>
<p>When we think variety, we think of the additional complexity that results from more kinds of data that we need to store, process, and combine.</p>
<p>Tables: could be in spreadsheets电子数据表 or databases or just files, but somehow they will be <strong>modeled and manipulated as rows and columns</strong> of tables.</p>
</blockquote>
<p><strong>Types</strong><br>Image data, text data, <strong>network data, geographic maps, computer generated simulations</strong> are only a few of the types of data we encounter everyday.</p>
<p><strong>The heterogeneity of data</strong><br>The heterogeneity[ˌhetərə’dʒə’ni:ətɪ] 异质性 can be characterized along several dimensions, we mentioned four such axes here: </p>
<ol>
<li>Structural variety:  the difference in the representation of the data.</li>
<li>Media variety: the medium in which the data gets delivered. Same information may be represented in two different media.</li>
<li>Semantic variety: </li>
</ol>
<ul>
<li>Using different units for quantities we measure.Or use qualitative versus quantitative measures, for example age can be a number or terms like infant, juvenile, or adult.</li>
<li>Different assumptions of conditions on the data, for example, two income surveys on two different groups.</li>
</ul>
<ol>
<li>Availability</li>
</ol>
<ul>
<li>real time V.S. can be stored</li>
<li>Similarly data can be accessible continuously V.S. Intermittently(间歇地), only something interest.</li>
</ul>
<p>Take the email as example: </p>
<ul>
<li><code>A single data object will not be all uniform in themselves</code>. Emails is a hybrid entity, it can be table, text, text may have  ornaments(highlight,markup), attachments like files, embedded images.</li>
<li>Collection from mailbox, or an organization.</li>
<li>Email collection have it’s own semantics.</li>
<li>Email Server is a real-time data source, but an email repository is not</li>
</ul>
<h3 id="Velocity"><a href="#Velocity" class="headerlink" title="Velocity"></a>Velocity</h3><blockquote>
<p>The increasing speed at which big data is created and the increasing speed at which the data needs to be stored and analyzed.</p>
<p>Match the speed of processing with the speed of information generation, and get real time decision making power. </p>
</blockquote>
<p><strong>batch processing:</strong> feeding data into machines and processed for days at a time.<br><strong>real time processing:</strong> </p>
<ul>
<li>Sensor-powered socioeconomic climate requires faster decisions. </li>
<li>Information is streaming and needs to be integrated with existing data to produce decisions,  We can not wait for all the data to be first produced, then fed into a machine. As more data comes in, the results will adapt to reflect this change.</li>
<li>Emergency response planning in a tornado, deciding trading strategies in real time, getting estimates in advertising,</li>
<li>Enables: cheap sensors technology, mobile phones, social media</li>
</ul>
<p><strong>The velocity of analytics</strong><br>The need for real time data-driven actions sometimes is a minute, sometimes half a day.</p>
<p>When timeliness of end result is an issue, you will have to make a decision based on <code>cost of hardware</code>, <code>time sensitivity of information</code>, <code>future scenarios</code>. This becomes a <strong>business driven</strong> question.</p>
<h3 id="Veracity"><a href="#Veracity" class="headerlink" title="Veracity"></a>Veracity</h3><blockquote>
<p>The quality of the data, sometimes gets referred to as validity or volatility 有效性或波动性 referring to the lifetime of the data.</p>
</blockquote>
<p><strong>Making big data operational</strong><br>Data can be noisy and uncertain, it can be full of <strong>biases</strong>, <strong>abnormalities</strong> and it can be <strong>imprecise</strong>. Junk in equals junk out. The evidence provided by data is only valuable if the data is of a satisfactory quality.</p>
<h4 id="Define-data-quality"><a href="#Define-data-quality" class="headerlink" title="Define data quality"></a>Define data quality</h4><p>Defined as a function of a couple off different variables: </p>
<ul>
<li>Accuracy of the data.</li>
<li>The trustworthiness or reliability of the data source.</li>
<li>How the data was generated.</li>
<li>How meaningful the data is with respect to the program that analyzes it.</li>
</ul>
<h4 id="The-reasons-for-uncertain"><a href="#The-reasons-for-uncertain" class="headerlink" title="The reasons for uncertain"></a>The reasons for uncertain</h4><ul>
<li>Unstructured data on the internet is imprecise and uncertain.</li>
<li>High velocity leaves very little or no time for the quality assurance processes.</li>
</ul>
<h4 id="Problem"><a href="#Problem" class="headerlink" title="Problem"></a>Problem</h4><p>The Google flu trends case from 2013, the news and social media attention paid to the particularly serious level of flu that year effected the estimate. Imagine the economical impact of making health care preparations for twice the amount of flu cases.<br>A banana slicer problem on amazon.com, because of fake reviewers.</p>
<h4 id="Challenges-on-data-quality"><a href="#Challenges-on-data-quality" class="headerlink" title="Challenges on data quality"></a>Challenges on data quality</h4><ul>
<li>What has been collected</li>
<li>Where it came from</li>
<li>How it was analyzed prior to its use</li>
</ul>
<h3 id="Valence"><a href="#Valence" class="headerlink" title="Valence"></a>Valence</h3><blockquote>
<p>Connectedness<br>The term valence comes from chemistry. valence electrons价电子 ,That higher valence results in greater boding, that is greater connectedness.</p>
</blockquote>
<h4 id="Feature"><a href="#Feature" class="headerlink" title="Feature"></a>Feature</h4><ul>
<li>Directly connected or Indirectly connected(Two scientists are connected because they are both physicists).</li>
<li>The data connectivity increases over time.</li>
</ul>
<h4 id="Measures"><a href="#Measures" class="headerlink" title="Measures"></a>Measures</h4><p> The ratio比值 of actually connected data items to the possible number of connections that could occur within the collection.</p>
<h4 id="Chellenge"><a href="#Chellenge" class="headerlink" title="Chellenge"></a>Chellenge</h4><ul>
<li><strong>Denser.</strong> A high valence data is denser. Makes many regular, analytic critiques very inefficient, more complex analytical methods must be adopted to account for the increasing density.</li>
<li><strong>Dynamic behavior</strong>. 1. Need a model and predict how valence of a connected data set may change with time and volume. 2. Leads to the problem of <strong>event detection</strong>. Such as bursts in the local cohesion in parts of the data, increased polarization in a community(社区极化)</li>
</ul>
<h3 id="Innovative-technologies"><a href="#Innovative-technologies" class="headerlink" title="Innovative technologies"></a>Innovative technologies</h3><p><strong>Analysis of Big Data</strong></p>
<ul>
<li>Parallel</li>
<li>Distributed computing paradigms</li>
<li>Scalable machine learning algorithms</li>
<li>Real-time querying</li>
</ul>
<p><strong>The Infrastructure for processing</strong></p>
<ul>
<li>Distributed file systems</li>
<li>Computing clusters</li>
<li>Cloud computing</li>
<li>Data stores supporting data variety and agility</li>
</ul>
<p><strong>Workflows</strong> provide an intuitive直观, reusable可重用, scalable可扩展 and reproducible可重复 way to process big data to gain verifiable可验证 value from it in and enable application of same methods to different datasets.</p>
<h2 id="Data-science"><a href="#Data-science" class="headerlink" title="Data science"></a>Data science</h2><h3 id="Data-science-1"><a href="#Data-science-1" class="headerlink" title="Data science"></a>Data science</h3><p>Turning data into insights or even actions. It is a basis for empirical research where data is used to induce information for observations.</p>
<p><strong>Observations:</strong> mainly data, in our case, big data, related to a business or scientific case.<br><strong>Insight:</strong> A term we use to refer to the data products of data science.<br><strong>Questions:</strong> Sometimes more specific, and sometimes it requires looking at the data and patterns in it to come up with the specific question.</p>
<p><strong>Data science is not static </strong><br>The process where <strong>models</strong> generated to lead to insights are <strong>constantly improved</strong> through further empirical evidence, or data.</p>
<p><strong>Data science teams</strong></p>
<ul>
<li>Coming from the breadth of information and skill</li>
</ul>
<ol>
<li>Happening at the intersection of <strong>computer science, mathematics and business expertise</strong>.</li>
<li>All of these circles require deeper knowledge and skills in areas like domain expertise, data engineering, statistics and computing.</li>
<li>Even deeper: Machine learning, statistical modeling, relational algebra关系代数, business passion, problem solving and data visualization.</li>
</ol>
<ul>
<li>Lots of <strong>moving parts</strong> to the solution, Data scientists have a combination of <strong>technical</strong>, <strong>business</strong> and <strong>soft</strong> skill. </li>
</ul>
<ol>
<li>Be relatively rare.</li>
<li>Be passionate about the story and the meaning behind data.</li>
<li>Understanding the problem, and aiming to solve this problem.</li>
<li>Having an interest in engineering solutions to solve problems.</li>
<li>Having curiosity about each others work, and having <strong>communication skills</strong> to interact with the team and <strong>present</strong> their ideas and results to others.</li>
</ol>
<h3 id="Big-Data-Strategy"><a href="#Big-Data-Strategy" class="headerlink" title="Big Data Strategy"></a>Big Data Strategy</h3><h4 id="Strategy"><a href="#Strategy" class="headerlink" title="Strategy"></a>Strategy</h4><p>A military term. A plan of action or policy designed to achieve a major or overall aim. Calling out the four major parts:</p>
<ul>
<li>Aim</li>
<li>Policy</li>
<li>Plan</li>
<li>Action</li>
</ul>
<h4 id="Big-data-Strategy"><a href="#Big-data-Strategy" class="headerlink" title="Big data Strategy"></a>Big data Strategy</h4><p>Iterate the strategy and make business more dynamic in the face of change.</p>
<ol>
<li>Define big objectives</li>
</ol>
<ul>
<li>Questions to turn big data into advantage for your business. </li>
<li>Both short term and long term objectives.</li>
<li>Be linked to big data analytics with business objectives. Each company needs to evaluate how data science would add value to their business objectives.</li>
</ul>
<ol>
<li>Create a culture to embrace it</li>
</ol>
<ul>
<li>The first and foremost ingredient is organizational buy-in.</li>
<li>Having commitment and sponsorship from the company’s leadership.</li>
<li>Goals should be developed with all stakeholders and clearly communicated to everyone in the organization.</li>
<li>Its value is understood and appreciated by all.</li>
</ul>
<ol>
<li>Build data science team</li>
</ol>
<ul>
<li>Necessary: data scientists, information technologists, application developers, and business owners<br><strong>- One for all: No one is a customer or service provider of another.</strong><code>???</code></li>
</ul>
<ol>
<li>Constant training of  on new tools and analytics, as well as business practices and objectives.</li>
</ol>
<ul>
<li>Because of team game and multi-disciplinary[ˌmʌltɪ’dɪsəplɪnerɪ] 多学科</li>
<li>Even more critical if your business depends on deep expertise on one or more subject areas with subject matter experts working on problems, utilizing big data.</li>
</ul>
<ol>
<li>A small data science team</li>
</ol>
<ul>
<li>Do data experiments</li>
<li>Test new ideas before they get deployed at full scale.</li>
<li>They take more research level role.</li>
<li>Their findings can drastically shape your business strategy almost on a daily basis.</li>
<li>They become strategic partners of all verticals in your business. Once you see that something works, you can start collection more data to see similar results at organizational scale.</li>
</ul>
<ol>
<li>Data silos</li>
</ol>
<ul>
<li>key, it is essential that data across the organization is easily accessed and integrated. </li>
<li>Remove barriers to data access and integration.Promote a data sharing mindset for the company. Organization’s leaders must encourage and support opening up the silos.</li>
</ul>
<ol>
<li>Defining the policies around big data. </li>
</ol>
<ul>
<li>Raise some concerns in long term planning for data.</li>
<li>What are the privacy concerns?</li>
<li>Who should have access to,or control data?</li>
<li>What is the lifetime of data?</li>
<li>Which is sometimes defined as volatility波动挥发, anatomy解析 of big data? </li>
<li>How does data get curated牧师，被净化 and cleaned up?</li>
<li>What ensures data quality in the long term?</li>
<li>How do different parts of your organization communicate or interoperate using this data?</li>
<li>Are there any legal and regulatory standards in place?</li>
</ul>
<ol>
<li>Cultivating an analytics driven culture</li>
</ol>
<ul>
<li>Analytics is an integral part of doing business, not a separate afterthought.</li>
<li>Analytics activities must be tied to your business objectives.</li>
<li>Analytics and business together bring about exciting opportunities and growth to your big data strategy.</li>
<li>One size does not fit all. Technologies and analytics is growing rapidly as your business is an evolving entity.</li>
</ul>
<h3 id="Five-P’s-of-data-science"><a href="#Five-P’s-of-data-science" class="headerlink" title="Five P’s of data science"></a>Five P’s of data science</h3><p><code>People</code> teaming up around application-specific <code>purpose</code> that can be achieved through a <code>process</code>, big data computing <code>platforms</code>, and <code>programmability</code>. Everything we do is to reach to that final <code>product</code> based on our purposes.</p>
<h4 id="Product"><a href="#Product" class="headerlink" title="Product"></a>Product</h4><p>Ending with the product.</p>
<p><strong>At beginning, asking “Lets not dive into the techniques yet, What is the problem at large? How do we see ourselves solving it? “</strong></p>
<h4 id="People"><a href="#People" class="headerlink" title="People"></a>People</h4><p>Referring to a data science team or the projects stakeholders.</p>
<h4 id="Purpose"><a href="#Purpose" class="headerlink" title="Purpose"></a>Purpose</h4><p>The challenge or set of challenges defined by big data strategy.</p>
<h4 id="Process"><a href="#Process" class="headerlink" title="Process"></a>Process</h4><p>People with purpose will define a process to collaborate and communication around.</p>
<p>The process is <strong>conceptual</strong> in the beginning and defines the set off steps on how everyone can contribute to it.</p>
<ol>
<li>One way: two distinct activities</li>
</ol>
<ul>
<li>Big data engineering</li>
<li>Big data analytics, or computational big data science.<br>skill.</li>
</ul>
<ol>
<li>More Detailed way: Five distinct steps that iterate<br>This process supports experimental work and dynamic scalability可测量性 on the big data and computing platforms.</li>
</ol>
<ul>
<li>Acquire</li>
<li>Prepare</li>
<li>Analyze</li>
<li>Report</li>
<li>Act</li>
</ul>
<p><strong>iterating process</strong><br>The influence of big data pushes for alternative <strong>scalability approaches</strong> at each step. All of these steps have reporting needs in different forms, or there is a need to draw all these activities as an <strong>iterating process</strong>, including build, explore, and scale for big data as steps.</p>
<p>Analysis needs alternative data management techniques and systems, as well as analytical tools and methods.</p>
<p>Multiple modes of scalability is needed based on dynamic data and computing loads.</p>
<p>Streaming data <strong>specific urgencies</strong> arising from special events can also require multiple modes of scalability.</p>
<h4 id="Platforms"><a href="#Platforms" class="headerlink" title="Platforms"></a>Platforms</h4><p>As a part of building process. Hadoop framework or other computing platforms to scale different steps.</p>
<p>The scalability should be in the mind of <strong>all team members</strong> and <strong>get communicated</strong> as an expectation.</p>
<h4 id="Programmability"><a href="#Programmability" class="headerlink" title="Programmability"></a>Programmability</h4><p>The scalable process should be programmable through utilization of <code>reusable</code> and <code>reproducible programming interfaces</code> to libraries, like systems middleware, analytical tools, visualization environments, and end user reporting environments.</p>
<h4 id="Accountability"><a href="#Accountability" class="headerlink" title="Accountability"></a>Accountability</h4><p>Expectations on cost, time, optimization of deliverables, and time lines.</p>
<p>Can be discussed between the members starting with the beginning of the data science process. But may not be able to do this in one step, and joint explorations like statistical evaluations of intermediate results or accuracy of sample data sets become important.</p>
<h4 id="Asking-the-right-questions"><a href="#Asking-the-right-questions" class="headerlink" title="Asking the right questions"></a>Asking the right questions</h4><p>The first step in any process is to define:</p>
<ul>
<li>What it is you are trying to tackle.</li>
<li>What is the problem that needs to be addressed</li>
<li>What is the opportunity that needs to be ascertained.</li>
</ul>
<p>By doing this, you can :</p>
<ul>
<li>Have a clear goal in mind.</li>
<li>Know when you’ve solved the problem.</li>
</ul>
<p>Firstly, ask a question.Next, you need to assess the situation:</p>
<ul>
<li>What are the requirements of the problem?</li>
<li>What are the assumptions and constraints?</li>
<li>What resources are available? (both personnel and capital, such as computer systems, instruments etc.)</li>
<li>What are the main costs associated with this project?</li>
<li>What are the potential benefits?</li>
<li>What risks are there in pursuing the project?</li>
<li>What are the contingencies意外事故，以备万一 to potential risks, and so on?</li>
</ul>
<p>Then define goals and objectives:</p>
<ul>
<li>Defining success criteria to clear goals and help you to access the project throughout its life cycle.</li>
<li>What do you hope to achieve by the end of this project?</li>
</ul>
<blockquote>
<p><strong>As a summary, having a <code>process</code> within the more business-driven P’s, like <code>people</code> and <code>purpose</code>, and the more technically driven P’s, like <code>platforms</code> and <code>programmability,</code> leads to a streamlined approach that starts and ends with the <code>product</code>, team <code>accountability</code>, and <code>collaboration</code> in mind.</strong></p>
</blockquote>
<h3 id="Data-Science-Process"><a href="#Data-Science-Process" class="headerlink" title="Data Science Process"></a>Data Science Process</h3><ol>
<li>Acquire: Including anything that makes us retrieve data including finding, accessing, acquiring, and moving data.</li>
<li>Prepare: Be divided into two steps: explore data(quality and format) and pre-process data(cleaning data, sub-setting or filtering data, creating data, integration of multiple data sources).<code>Spend most of time, garbage in,garbage out</code></li>
<li>Analysis: selection of analytical techniques to use, building a model of the data, and analyzing results.</li>
<li>Communication results: Evaluation off analytical results, interpret, summarize, visualize, or post process.</li>
<li>Turning Insights into Action: based on the purpose.</li>
</ol>
<h4 id="Acquiring-data"><a href="#Acquiring-data" class="headerlink" title="Acquiring data"></a>Acquiring data</h4><p>Finding and  evaluating data useful to analytics is important before you start acquiring data.<br> <strong>Determining what data is available</strong></p>
<ul>
<li>Identify <strong>suitable data related</strong> to the problem</li>
<li>Make use of <strong>all data</strong> that is relevant to the problem</li>
</ul>
<p><strong>Techniques</strong><br>Data source: many place: Local and remote; in many varieties: structured and un-structured; with different velocities.</p>
<p>Depending on the source and structure of data, there are alternate ways to access it.</p>
<ul>
<li>Conventional relational databases</li>
</ul>
<ol>
<li><strong>SQL</strong>: Supported by all relational databases management systems.</li>
<li><strong>Graphical application environment</strong>: Most database systems come with it, which allows you to query and explore the data sets in the database.</li>
</ol>
<ul>
<li>Files</li>
</ul>
<ol>
<li><strong>Scripting languages:</strong><br>High level programming language that can be either general purpose通用功能 or specialized for specific functions.<br><br>Common: Java Script, Python, PHP, Perl, R, and MATLAB…</li>
</ol>
<ul>
<li>Websites</li>
</ul>
<ol>
<li><strong>Web services:</strong><br>Many websites host web services which produce program access to their data. <br><br><strong>REST:</strong> Representational State Transfer, a type of web services, easy to use. It is an approach to implementing实现 web services with performance性能, scalability扩展性 and maintainability可维护性 in mind.</li>
<li><strong>Web socket services</strong><br>Allow real time modifications更改 from web sites.</li>
</ol>
<ul>
<li>NoSQL storage systems<br>Used to manage a variety of data types in big data. For example, Cassandra, MongoDB and HBASE.</li>
</ul>
<ol>
<li><strong>APIs</strong>: NoSQL data stores provide APIs to allow users to access data. Can be used directly or in an application.</li>
<li><strong>Web service interface</strong>: most NoSQL system provide, such as REST.</li>
</ol>
<p><strong>Wildfire case study</strong><br>The combination of sensor data and tweet sentiments helps to give us a sense of the urgency of the fire situation.</p>
<ul>
<li>Sensor data from weather stations in a relational database:<br>Using <strong>SQL</strong> to retrieve data and create models to identify weather patterns associated with Santa Anna conditions.</li>
<li>Determine whether a weather station is currently experiencing Santa Anna conditions:<br>Using a <strong>web socket service</strong> to access real time data.</li>
<li>Tweet messages<br>Using the <strong>Twitter REST service</strong>. </li>
</ul>
<h4 id="Prepare-1-Exploring-Data"><a href="#Prepare-1-Exploring-Data" class="headerlink" title="Prepare 1: Exploring Data"></a>Prepare 1: Exploring Data</h4><p>Do some preliminary初步的 investigation in order to gain a better understanding of the specific characteristics of your data. This will guide the rest of your process.<br><strong>Correlation graph: </strong>dependencies between variables<br><strong>General trends:  </strong>consistent direction<br><strong>Outliers: </strong>异常点, double check for errors or find a rare event.<br><strong>Summary statistics: </strong>quantities that capture various characteristics of a set of values with a single numerical values.</p>
<ul>
<li>The location of a set of valuse: mean, median</li>
<li>Most frequently: Mode</li>
<li>The spread: range, standard deviation<br><strong>Visualization techniques: </strong></li>
<li>Heat map: where the hotspots are.</li>
<li>Histograms: data distribution, show skewness偏度 or unusual dispersion异常离差</li>
<li>Boxplots: data distribution</li>
<li>Line graphs: change over time, spikes尖峰</li>
<li>Scatter plots: 散点图,correlation between two variables</li>
</ul>
<h4 id="Prepare-2-Pre-Processing-Data"><a href="#Prepare-2-Pre-Processing-Data" class="headerlink" title="Prepare 2: Pre-Processing Data"></a>Prepare 2: Pre-Processing Data</h4><p><strong>Goals</strong></p>
<ul>
<li>Quality issues: Clean the data to address data quality issues.</li>
<li>Data manipulation: Transform the raw data to make it suitable for analysis.</li>
</ul>
<p><strong>Quality issues</strong></p>
<ul>
<li>Inconsistent data: like a customer with two different addresses, duplicate复制 records and the two recording don’t agree.</li>
<li>Missing values</li>
<li>Invalid data: like an invalid zip code</li>
<li>outliers</li>
</ul>
<p>Some approaches to address quality issues:</p>
<ul>
<li>Remove data records with missing values.</li>
<li>Merge duplicate records, requiring a  way to determine how to resolve conflicting矛盾 values, like retain the newer value.</li>
<li>For invalid values, the best <strong>estimate</strong> for a reasonable value can be used as a replacement. For example, age value can be estimated based on the employee’s length of employment.</li>
<li>Remove outliers if they are not important.</li>
</ul>
<p><strong>Domain knowledge</strong> is essential to making informed decisions on how to handle incomplete or incorrect data: how the data was collected, the user population, and the intended used of the application.</p>
<p><strong>Data manipulation: </strong><br>Manipulate the clean data into the format needed for analysis, including scaling, transformation, feature selection, dimensionality reduction, data manipulation.</p>
<ul>
<li>Scaling: changing the range of values to be between a specified range, to avoid large values from dominating the results. For example, scaling all values will <strong>equalize contributions from both height and weight features.</strong></li>
<li>Transformation: reduce noise and variability变化性，不确定.</li>
</ul>
<ol>
<li>Aggregation集成: aggregation daily values to weekly or monthly.</li>
<li>Other filtering techniques: remove variability, at the cost of less detailed data, so these factors must be weighted for the specific application.</li>
</ol>
<ul>
<li>Feature selection</li>
</ul>
<ol>
<li>Removing redundant or irrelevant features: correlated features like the purchase price and the amount of sales tax paid.</li>
<li>Combining features or Creating new features: There are algorithms to automatically determine the most relevant features.</li>
</ol>
<ul>
<li>Dimensionality reduction: Finding a smaller subset of dimensions that captures most of the variation. Commonly use PCA(principle component analysts)</li>
<li>Data manipulation: to be in the correct format for analysis.For a particular need, grouping the data required together, computing the mean, range, standard deviation for each group.</li>
</ul>
<h4 id="Analyzing-Data"><a href="#Analyzing-Data" class="headerlink" title="Analyzing Data"></a>Analyzing Data</h4><p><strong>analysis techniques</strong></p>
<ul>
<li><strong>Classification: </strong> predict the category.</li>
<li><strong>Regression: </strong>predict a numeric value.</li>
<li><strong>Clustering: </strong>organize similar items into groups.</li>
<li><strong>Association analysis: </strong>come up with a set off rules to capture associations within items or events. Market basket analysis can be used for cross-selling, discover a connection between two seemingly unrelated products, like diaper beer connection.</li>
<li><strong>Graph analysis: </strong>Having a lot of entities and connections between those entities. Exploring the spread of a disease, identification of security threats.</li>
</ul>
<p><strong>Evaluate technique</strong><br>Evaluating the model depends on the type of analysis techniques.</p>
<ul>
<li>Compared with correct output: For classification and regression.</li>
<li>Examined to see if it make sense for the application: For clustering. Are they helpful? </li>
<li>Do investigation: For association analysis and graph analysis.</li>
</ul>
<p><strong>Next steps</strong></p>
<ul>
<li>Should the analysis be performed with more data in order to get a better model performance?</li>
<li>Would using different data types help?</li>
<li>Do the analysis results suggest a more detailed look at some aspect of the problem? For example, predicting sunny weather gives very good results, but rainy weather predictions are just so-so, perhaps less samples of rainy weather, or some anomalies, or missing data.</li>
<li>Ready to move on? The model platforms very well with respect to the success criteria that were determined when you defined the problem at the beginning of the project.</li>
</ul>
<h4 id="Communicating-Results"><a href="#Communicating-Results" class="headerlink" title="Communicating Results"></a>Communicating Results</h4><p>Make a case for what actions should follow. It can change shape based on your audience.</p>
<p>We should have tables with details from your analysis as backups, if someone wants to take a deeper dive into the results.</p>
<p><strong>What to present</strong></p>
<ul>
<li>What is the punchline点睛之笔? In other words, What are the main results?</li>
<li>What added value do these results provide or how can the model add to the application?</li>
<li>How do the results compare to the success criteria determined at the beginning?</li>
<li>All findings must be presented so that informed decisions can be made, including results that are inconclusive不确定无结果 or puzzling. Inconclusive findings may lead to additional analysis.</li>
</ul>
<p><strong>Visualization tools</strong></p>
<ul>
<li>R</li>
<li>Python</li>
<li>D3: a JavaScript library for producing interactive web based visualizations and data driven documents.</li>
<li>Leaflet: a lightweight mobile friendly JavaScript library to create interactive maps.</li>
<li>Tableau Public: can share, put them on a site, or blog.</li>
<li>Google Charts: provides cross-browser compatibility, and closed platform portability to iPhones and Android.</li>
<li>Timeline: a JavaScript library that allows you to create timelines.</li>
</ul>
<h4 id="Turning-Insights-into-Action"><a href="#Turning-Insights-into-Action" class="headerlink" title="Turning Insights into Action"></a>Turning Insights into Action</h4><p>Only be useful if the insights can be turned into action, and if the actions are carefully defined and <code>evaluated</code>.</p>
<p><strong>Find actionable insights: </strong> for improving business processes.</p>
<ul>
<li>Is there something in your process that should change to <code>remove bottle necks</code>瓶颈?</li>
<li>Is there data that should be added to your application to make it <code>more accurate</code>?</li>
<li>Should you segment your population into more well defined groups for more <code>effective targeted marketing</code>?</li>
</ul>
<p><strong>Implement the action</strong></p>
<ul>
<li>What is necessary to add this action into your process or application?</li>
<li>How should it be <code>automated</code>?</li>
<li>The <code>stakeholders</code> need to be identified and become involved in this change.</li>
<li>Monitor and measure the <code>impact of the action</code> on the process or application.</li>
</ul>
<p><strong>Evaluation</strong><br>Evaluating results from the implemented action will determine the next steps.</p>
<ul>
<li>Is there <code>additional analysis</code> that need to be performed in order to yield even better results?</li>
<li>What data should be <code>revisited</code>?</li>
<li>Are there additional opportunities that should be explored?<br>for example: <strong>Real-time actions</strong></li>
</ul>
<ol>
<li>Define <code>what part of business</code> needs real-time action to be able to influence the operations or the interaction with the customer.</li>
<li>Make sure that there are automated systems, or processes to perform such actions, and provide <code>failure recovery</code> in case of problems.</li>
</ol>
<h2 id="Basic-Scalable-Computing-Concepts"><a href="#Basic-Scalable-Computing-Concepts" class="headerlink" title="Basic Scalable Computing Concepts"></a>Basic Scalable Computing Concepts</h2><h3 id="Distributed-File-System"><a href="#Distributed-File-System" class="headerlink" title="Distributed File System"></a>Distributed File System</h3><p><strong>File system:</strong> How the operating system manages files, be responsible from the organization of the long term information storage.<br><strong>Distributed file system:</strong> many storage computers are connected through the network. It provide data<code>scalability</code>, <code>fault tolerance</code>, and <code>high concurrency</code> through partitioning and replication of data on many nodes.</p>
<h3 id="Scalable-computing"><a href="#Scalable-computing" class="headerlink" title="Scalable computing"></a>Scalable computing</h3><p>The computation needs more than a node or parallel processing.</p>
<h3 id="Parallel-computers"><a href="#Parallel-computers" class="headerlink" title="Parallel computers"></a>Parallel computers</h3><p>A very large number of single computing nodes with <strong>specialized capabilities</strong> connected to other network. It’s pretty <strong>costly</strong>.</p>
<h4 id="Commodity-cluster"><a href="#Commodity-cluster" class="headerlink" title="Commodity cluster"></a>Commodity cluster</h4><p>Be <code>affordable parallel computers</code> with <code>Less-specialized</code> nodes.<br>The service-oriented computing community has pushed for <code>distributed computing</code> over the Internet, and in turn, <code>reduced computing cost</code>.<br><strong>Architecture</strong><br>Enable <code>data-parallelism</code>.</p>
<ul>
<li>In commodity cluster, the computing nodes are clustered in <code>racks</code> connected to each other via a fast <code>network</code>.</li>
<li>Distributed computing: computing in one or more of these clusters across a local area network or the internet.</li>
</ul>
<p><strong>Common failures</strong></p>
<ul>
<li>A node, or an entire rack can fail at any given time.</li>
<li>The connectivity of a rack to the network can stop </li>
<li>The connections between individual nodes can break.</li>
</ul>
<p><strong>Fault-tolerance</strong><br>It is not practical to restart everything every time, there are two neat solutions:</p>
<ul>
<li>Redundant data storage</li>
<li>Restart of failed individual parallel jobs.</li>
</ul>
<h3 id="Programming-Models"><a href="#Programming-Models" class="headerlink" title="Programming Models"></a>Programming Models</h3><p>A set of abstract runtime libraries运行库 and programming languages that form a model of computation.</p>
<p>The abstraction level can be low-level as in machine language, or high-level like Java.</p>
<h4 id="The-Requirements"><a href="#The-Requirements" class="headerlink" title="The Requirements"></a>The Requirements</h4><ul>
<li>Support common big data operations</li>
</ul>
<ol>
<li>Split volumes of data分割大量数据. Synchronize同步 the datasets later on</li>
<li>Access data fast.</li>
<li>Distribute computations to nodes</li>
</ol>
<ul>
<li>Handle Fault Tolerance</li>
</ul>
<ol>
<li>Replicate data partitions</li>
<li>Recover files when needed</li>
</ol>
<ul>
<li>Enable Adding More Racks</li>
</ul>
<ol>
<li>Be easily scalable to the distributed notes where the data gets produced.</li>
<li>Enable adding new resources and scale to more or faster data without losing performance. </li>
</ol>
<ul>
<li>Optimized for specific data types: at least one type.</li>
</ul>
<h4 id="MapReduce"><a href="#MapReduce" class="headerlink" title="MapReduce"></a>MapReduce</h4><p>A programming model for Big Data, it abstracts out the details of parallelzation, full tolerance, data distribution, <strong>monitoring</strong> and <strong>load balancing</strong>.<br>Many implementations, like Hadoop.</p>
<h2 id="Hadoop"><a href="#Hadoop" class="headerlink" title="Hadoop"></a>Hadoop</h2><h3 id="Overarching-themes-and-goals"><a href="#Overarching-themes-and-goals" class="headerlink" title="Overarching themes and goals"></a>Overarching themes and goals</h3><ul>
<li>Provide scalability to store large volumes of data on commodity hardware.</li>
<li>The ability to gracefully recover from crashes as the increasing on systems, to supported by most frameworks in the Hadoop ecosystem.</li>
<li>The ability to handle different data types. For any given type of data, you can find several projects in the ecosystem that support it.</li>
<li>The ability to facilitate a shared environment.</li>
<li>Providing value for your enterprise. Open source projects.</li>
</ul>
<h3 id="The-Hadoop-Ecosystem"><a href="#The-Hadoop-Ecosystem" class="headerlink" title="The Hadoop Ecosystem"></a>The Hadoop Ecosystem</h3><p>The layer diagram is organized vertically based on the <code>interface</code>. Low level interfaces, so <code>storage</code> and <code>scheduling</code>, on the bottom. And high level <code>languages</code> and <code>interactivity</code> at the top.  </p>
<ul>
<li><strong>HDFS</strong>(Hadoop distributed file system): the foundation, providing <code>scaleable and reliable</code> storage.</li>
<li><strong>YARN</strong>: providing <code>flexible scheduling</code> and <code>resource management</code> over the HDFS storage, not just for MapReduce but other programming models as well. Be used at Yahoo to schedule jobs across 40000 servers.</li>
<li><strong>MapReduce</strong>: a programming model that simplifies parallel computing. </li>
</ul>
<p>MapReduce Only assume a limited model to express data, Hive and Pig are two additional programming models on top of MapReduce to <code>augment增加 data modeling</code> of MapReduce with <code>relational algebra</code> and <code>data flow modeling</code> respectively分别.</p>
<ul>
<li><strong>Hive</strong>: Be created at Facebook to issue <code>SQL-like</code> queries using MapReduce on their data in HDFS.</li>
<li><strong>Pig</strong>: Be created at Yahoo to model <code>data flow</code> based programs using MapReduce.</li>
<li><strong>Giraph</strong>: Be build for processing large-scale graphs efficiently. Facebool uses it to analyze the social graphs of its users.</li>
<li><strong>Storm, Spark, and Flink</strong>: For real time and in memory processing of big data on top of the YARN resource scheduler and HDFS.</li>
</ul>
<p>Some data are not efficiently represented using the file and directory model of storage, like collections of key-values or large sparse稀疏 tables. <code>NoSQL projects</code> such as <strong>Cassandra, MongoDB, and HBase</strong> handle these cases.</p>
<ul>
<li><strong>Cassandra</strong>: Be created at Facebook, but Facebook also used HBase for its messaging platform.</li>
<li><strong>Zookeeper</strong>: A centralized management system for synchronization, configuration配置 and to ensure high availability效率. Be created by Yahoo.</li>
</ul>
<h3 id="HDFS"><a href="#HDFS" class="headerlink" title="HDFS"></a>HDFS</h3><h4 id="Two-capabilities"><a href="#Two-capabilities" class="headerlink" title="Two capabilities"></a>Two capabilities</h4><p><code>Scalability</code> to large data sets HDFS splits files across nodes for parallel access </p>
<ul>
<li>Typical file size is GBs to TBs. </li>
<li>Default chunk size, the size of each piece of a file is 64MBs.<br><code>reliability</code> to cope with hardware failures. Replication for fault tolerance. By default, HDFS maintains three copies of every block, in different geographical locations <h4 id="Two-key-components"><a href="#Two-key-components" class="headerlink" title="Two key components"></a>Two key components</h4>Two components using a master slave relationship. </li>
<li><code>NameNode</code> for metadata: Usually one per cluster, and coordinating operations: </li>
</ul>
<ol>
<li>Keeps track of file name, location in directory, etc. </li>
<li>Mapping of contents on DataNode. </li>
</ol>
<ul>
<li><code>DataNode</code> for block storage: Usually one per machine, and <strong>listens to NameNode</strong> for block creation, deletion, replication(provides two capabilities: Fault Tolerance, Data locality). </li>
</ul>
<h3 id="YARN"><a href="#YARN" class="headerlink" title="YARN"></a>YARN</h3><p>A resource manage layer that sits just above the storage layer HDFS. It reduces the need to move data around and supports <strong>higher resource utilization</strong> resulting in lower costs, and lets you <strong>run many distributed applications over the same Hadoop cluster</strong> by providing a standard framework that supports customized定制 application development in the Hadoop ecosystem.</p>
<h4 id="Share-Hadoop-across-applications"><a href="#Share-Hadoop-across-applications" class="headerlink" title="Share Hadoop across applications"></a>Share Hadoop across applications</h4><ul>
<li>YARN <strong>interacts with applications</strong> and <strong>schedules resources</strong> for their use. </li>
<li><code>Support non-MapReduce applications</code>: Giraph for graph data analysis, Storm for streaming data analysis, and Spark for in-memory analysis.</li>
<li>One dataset for many applications </li>
</ul>
<h4 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h4><p><code>Resource manager</code>decides who gets what<br><code>Node manager</code>operates at machine level and in charge of a single machine.<br><code>Container</code>: an abstract Notions概念 that signifies所指 a resource that is a collection of CPU memory disk network and other resources within the compute note.</p>
<p>Together the resource manager and the node manager form the data <code>computation framework</code>. Each application gets an <code>application master</code>, which <strong>negotiate协商 resource from the Resource Manager</strong> and it <strong>talks to Node Manager to get its tasks completed</strong>.</p>
<h3 id="MapReduce-1"><a href="#MapReduce-1" class="headerlink" title="MapReduce"></a>MapReduce</h3><p>MapReduce relies on YARN to schedule and execute<code>**parallel processing**</code>over the distributed file blocks in HDFS.</p>
<p><strong>Traditional parallel programming</strong></p>
<ul>
<li>Requires synchronization mechanisms like locks, semaphores, and monitors(像锁、信号量和监视器这样的同步机制).</li>
<li>It is error prone, since your code can run on many nodes, each having many cores. And any problem related to these parallel processes, needs to be handled by your parallel program.</li>
</ul>
<p>Because of MapReduce, we don’t have to worry about multiple threads, synchronization, or concurrency issues.</p>
<p><strong>Higher level tools</strong></p>
<ul>
<li><strong>Hive</strong> has a <strong>SQL-like interface</strong> that adds capabilities that help with <strong>relational data modeling</strong>.</li>
<li><strong>Pig</strong> is a high level data flow language that adds capabilities that help with process <strong>map modeling</strong>.</li>
</ul>
<h4 id="Map-and-Reduce"><a href="#Map-and-Reduce" class="headerlink" title="Map and Reduce"></a>Map and Reduce</h4><p>For map, the operation is applied on each data element.<br>In reduce, the operation summarizes elements in some manner.</p>
<h4 id="WordCount"><a href="#WordCount" class="headerlink" title="WordCount"></a>WordCount</h4><p>There are three distinct steps, Namely, the map step, the shuffle and sort step, and the reduce step.</p>
<p>For simplification we are assuming we have one big file as an input.The input file is stored in HDFS, which partitions the blocks across multiple nodes in the cluster. In this case, four partitions labeled A, B, C and D. </p>
<ol>
<li>The map step: The first step is to run a map operation on each node. </li>
</ol>
<ul>
<li>As the input partitions are read from HTFS, map is called for each line in the input.</li>
<li>The first line, in the partition on node A, says, My apple is red and my rose is blue. Map creates a key value for each word on the line containing the word as the key, and 1 as the value. In this example, the word apple is read from the line in partition A, Map produces a key value of (apple 1). </li>
<li>The same map operation generates for other partitions. A list of all the words with one key-value pairing each gets generated. </li>
<li>Moving computation to data. Note that map goes to each node containing a data block for the file, instead of the data moving to map.</li>
</ul>
<ol>
<li>The shuffle and sort step</li>
</ol>
<ul>
<li>All the key-values that were output from map are sorted based on their key. </li>
<li>And the key values, with the same word, are moved, or shuffled, to the same node. </li>
<li>In general, a node will have many different words.</li>
</ul>
<ol>
<li>The reduce step</li>
</ol>
<ul>
<li>The reduce operation executes on these nodes to add values for key-value pairs with the same keys. For example, (apple,1) and another (apple,1), becomes (apple,2). </li>
<li>The result of reduce is a single key pair for each word that was read in the input file. The key is the word, and the value is the number of occurrences.</li>
</ul>
<p><strong>Simple example?</strong><br>It can be used in search engines. Consider changing the WordCount algorithm to index all the URLs by words after a web crawl. </p>
<ul>
<li>This means, instead of pointing to a number, the keys would refer to URLs.</li>
<li>This is one of the ways a search engine like Google works.</li>
<li>So now, if somebody came to the interface to search for the woed, it would be easy to get all the URLs as the word itself.</li>
</ul>
<h4 id="The-points-of-data-parallelism"><a href="#The-points-of-data-parallelism" class="headerlink" title="The points of data parallelism"></a>The points of data parallelism</h4><ol>
<li>This parallelization is over the input</li>
</ol>
<ul>
<li>As each partition gets processed one line at a time.</li>
<li>We must decide on the data granularity(粒度) of each parallel competition. In the case of WordCount, it will be a line.</li>
</ul>
<ol>
<li>There is definitely parallelization during the map step.</li>
<li>Parallel grouping of data in the shuffle and sort phase. </li>
</ol>
<ul>
<li>The parallelization is over the intermediate products, that is, the individual key-value pairs.</li>
</ul>
<ol>
<li>The reduce step gets parallelized to construct one output file.</li>
</ol>
<h4 id="The-tasks-that-MapReduce-is-BAD-for"><a href="#The-tasks-that-MapReduce-is-BAD-for" class="headerlink" title="The tasks that MapReduce is BAD for"></a>The tasks that MapReduce is BAD for</h4><p>MapReduce excels at independent batch tasks.,include <strong>search engine page ranking and topic mapping</strong> There are certain kinds of tasks that you would not want to use MapReduce for.</p>
<ul>
<li><strong>The data is frequently changing.</strong> MapReduce is slow since it reads the entire input data set each time.</li>
<li><strong>Computations that do have dependencies</strong>, cannot be expressed with MapReduce.</li>
<li><strong>It unsuitable for interactive applications</strong> where the results must be presented to the user very quickly, expecting a return from the user. Because MapReduce does not return any results until the entire process is finished. <h3 id="Reconsider-Hadoop"><a href="#Reconsider-Hadoop" class="headerlink" title="Reconsider Hadoop"></a>Reconsider Hadoop</h3>Although Hadoop is good with scalability of many algorithms, it is just one model and does not solve all issues in managing and processing big data.<h4 id="The-key-features-that-make-a-problem-Hadoop-friendly"><a href="#The-key-features-that-make-a-problem-Hadoop-friendly" class="headerlink" title="The key features that make a problem Hadoop friendly"></a>The key features that make a problem Hadoop friendly</h4></li>
<li>A large scale growth in amount of data. Probably it make sense to use Hadoop.</li>
<li>When you want quick access to your old data which would otherwise go on tape drives for archival storage文件存储, Hadoop might provide a good alternative.</li>
<li>When you want to use multiple applications over the same data store.</li>
<li>High volume or high variety are also great indicators for Hadoop as a platform choice.</li>
</ul>
<h4 id="The-Hadoop-framework-is-not-the-best-for"><a href="#The-Hadoop-framework-is-not-the-best-for" class="headerlink" title="The Hadoop framework is not the best for:"></a>The Hadoop framework is not the best for:</h4><ul>
<li>Small data set</li>
<li>Advanced algorithms that require a specific hardware type.</li>
<li>Task-level parallelism. Hadoop is good for data parallelism.<blockquote>
<p><strong>Data parallelism</strong>: the simultaneous execution of the same function on multiple nodes across the elements of a dataset.</p>
<p><strong>Task parallelism</strong>: The simultaneous execution of many different functions on multiple nodes across the same or different data sets.</p>
</blockquote>
</li>
</ul>
<ul>
<li>Infrastructure replacement.</li>
<li>Random data access. Because HDFS stores data in blocks of 64MBs or larger, so you may have to read an entire file just to pick one data entry.</li>
</ul>
<h4 id="Some-of-the-moving-targets-in-the-Hadoop-ecosystem"><a href="#Some-of-the-moving-targets-in-the-Hadoop-ecosystem" class="headerlink" title="Some of the moving targets in the Hadoop ecosystem"></a>Some of the moving targets in the Hadoop ecosystem</h4><ul>
<li>Advanced analytical queries.查询</li>
<li>Latency sensitive tasks 延迟敏感任务.</li>
<li>Cyber security of sensitive data.<h3 id="Cloud-Computing"><a href="#Cloud-Computing" class="headerlink" title="Cloud Computing"></a>Cloud Computing</h3>Cloud Computing is an important Big Data Enabler. We call it on-demand computing.</li>
</ul>
<p>The main idea behind cloud computing is to transform computing infrastructure into a commodity. So application developers can focus on solving application specific challenges instead of trying to build infrastructure to run on. </p>
<p>Cloud provides convenient and viable可行 solutions for scaling扩展 your prototype to a full fledged成熟 application. You can leverage利用 the experts to handle security, robustness健壮性, and let them handle the technical issues.</p>
<h4 id="In-house-capabilities"><a href="#In-house-capabilities" class="headerlink" title="In-house capabilities"></a>In-house capabilities</h4><p>Building your own data center or computing power house can be <strong>expensive</strong>. And it can be <strong>time comsuming</strong>. This requires <strong>high initial capital investments</strong> and <strong>efficient operation of several departments</strong> in your business. Most people forget to include <strong>the cost of disposing old hardware</strong>.</p>
<ul>
<li>You have to hire people</li>
<li>Buy hardware that suits your requirements. </li>
</ul>
<ol>
<li>These includes, but not limited to, buying networking hardware, storage disks, upgrading hardware when it becomes obsolete, and so on. </li>
<li>How do you estimate the size of your hardware needs? It is becoming harder to estimate future demands.</li>
</ol>
<ul>
<li>Getting the software that fits your needs is equally challenging.</li>
</ul>
<ol>
<li>Most software installations require a lot of tweaking and manual intervention that require a lot of skills. You will need your engineers to do this.</li>
<li>Compatibility(通用性) issues bring problems that are hard to foresee.</li>
<li>You must ensure you’re updated.</li>
</ol>
<h4 id="Cloud-computing-service"><a href="#Cloud-computing-service" class="headerlink" title="Cloud computing service"></a>Cloud computing service</h4><p>We can simply define a cloud computing service, as a rental service for computing. You rent what you want, and return upon usage. Get what you want, and pay for what you use.</p>
<ul>
<li>You pay for what you use.</li>
</ul>
<ol>
<li>which means a low capital investment.</li>
<li>You don’t need to go to the dealership经销商, do a negotiation谈判, get a bank loan, get insurance. That means quick implementation of your projects.</li>
</ol>
<ul>
<li>Adapt to your requirements faster if your business is growing faster than you thought.</li>
<li>Cloud lets you forget about the resource management problems and lets you focus on your business’s products or domain expertise with minimal cost.</li>
<li>You can build your own custom machine on cloud.</li>
</ul>
<ol>
<li>Custom machine is a commodity cluster made out of the right type of computing nodes for your application.</li>
<li>You pick not only a CPU or a GPU, but pick from a whole menu of compute, memory and storage choices.</li>
<li>Design machines to suit your application requirements, data size and analytics.</li>
</ol>
<h4 id="Three-main-class-service-models"><a href="#Three-main-class-service-models" class="headerlink" title="Three main class service models"></a>Three main class service models</h4><ul>
<li>Infrastructure as a service (IaaS): Amazon EC2</li>
<li>Platform as a service (PaaS)</li>
</ul>
<ol>
<li>A user is provided with an entire computing platform. This could include the operating system and programming languages.</li>
<li>It could extend to include the database of your choice, or even a web server. You can develop, and run your own application software, on top of these layers.</li>
<li>The Google App engine and Microsoft Azure are two examples of this model.</li>
</ol>
<ul>
<li>Software as a service (SaaS)</li>
</ul>
<ol>
<li>The cloud service provider takes the responsibilities for the hardware and software environment such as the operating system and the application software.</li>
<li>Dropbox is a very popular software as a service platform.</li>
</ol>
<ul>
<li>XaaS</li>
</ul>
<ol>
<li>XaaS is an umbrella term that signifies even finer-grain control(细粒度控制) over computing resources that you want to rent.</li>
<li>For example, storage as a service, communication as a service, marketing as a service, and so on.</li>
</ol>
<ul>
<li>Which service to explore</li>
</ul>
<ol>
<li>The skill level of your team to handle computing environment, development and maintenance.</li>
<li>How you might need to use the service.</li>
<li>Capital.</li>
<li>Best fits you in terms of long term goals.</li>
<li>Understand all the security risks.  Since your data resides on third party service.You must make your client’s data safety a top priority.</li>
</ol>
<h3 id="Pre-built-Hadoop-images"><a href="#Pre-built-Hadoop-images" class="headerlink" title="Pre-built Hadoop images"></a>Pre-built Hadoop images</h3><p>Assembling your own software stack from scratch can be messy and a lot of work for beginners. The task of setting up the whole stack could consume a lot of project time and man power, reduce time to deployment.</p>
<p>Getting pre-built images is similar to buying pre-assembled furniture. You can obtain a ready to go software stack which contains a pre-installed operating system, required libraries and application software.</p>
<p>It saves you from the trouble of putting the different parts togerther in the right orientation. You can start using the furniture right away.</p>
<h4 id="virtualization-software"><a href="#virtualization-software" class="headerlink" title="virtualization software"></a>virtualization software</h4><p>Packaging of these pre-built software images is enabled by virtual machines using virtualization software.<br>Your software stack comes as a large file. Virtualization software provides a platform where your stack can run.</p>
<h4 id="Companies"><a href="#Companies" class="headerlink" title="Companies"></a>Companies</h4><ul>
<li>Hortonworks: providing a pre-built software stack for both Mac and Windows platforms.</li>
<li>Cloudera: Providing pre-installed and assembled software stack images.</li>
</ul>

        
        <br />
        <div id="comment-container">
        </div>
        <div id="disqus_thread"></div>

        <div id="lv-container">
        </div>

    </div>
</div>

    </div>
</div>


<footer class="footer">
    <ul class="list-inline text-center">
        
        

        

        

        

        

    </ul>
    
    <p>
        <span id="busuanzi_container_site_pv">
            <span id="busuanzi_value_site_pv"></span>PV
        </span>
        <span id="busuanzi_container_site_uv">
            <span id="busuanzi_value_site_uv"></span>UV
        </span>
        Created By <a href="https://hexo.io/">Hexo</a>  Theme <a href="https://github.com/aircloud/hexo-theme-aircloud">AirCloud</a></p>
</footer>




<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });

</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<!-- <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> -->

</body>

<script>
    // We expose some of the variables needed by the front end
    window.hexo_search_path = "search.json"
    window.hexo_root = "/junlian.github.io/"
    window.isPost = true
</script>
<script src="https://cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script>
<script src="/junlian.github.io/js/index.js"></script>
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




</html>
