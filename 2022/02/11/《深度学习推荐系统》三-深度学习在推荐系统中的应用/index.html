<!DOCTYPE html>
<html lang="en">
<head><meta name="generator" content="Hexo 3.9.0">
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="google-site-verification" content="xBT4GhYoi5qRD5tr338pgPM5OWHHIDR6mNg1a3euekI">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Juliet&#39;s blog">
    <meta name="keyword" content>
    <link rel="shortcut icon" href="/junlian.github.io/img/favicon.ico">

    <title>
        
        《深度学习推荐系统》三 深度学习在推荐系统中的应用 - 学 | 慢慢来
        
    </title>

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/junlian.github.io/css/aircloud.css">
    <link rel="stylesheet" href="/junlian.github.io/css/gitment.css">
    <!--<link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css">-->
    <link href="//at.alicdn.com/t/font_620856_pl6z7sid89qkt9.css" rel="stylesheet" type="text/css">
    <!-- ga & ba script hoook -->
    <script></script>
</head>

<body>

<div class="site-nav-toggle" id="site-nav-toggle">
    <button>
        <span class="btn-bar"></span>
        <span class="btn-bar"></span>
        <span class="btn-bar"></span>
    </button>
</div>

<div class="index-about">
    <i> 所谓诚其意者，毋自欺也，如恶恶臭，如好好色，此之谓自谦，故君子必慎其独也。 ——摘《大学》 </i>
</div>

<div class="index-container">
    
    <div class="index-left">
        
<div class="nav" id="nav">
    <div class="avatar-name">
        <div class="avatar ">
            <img src="/junlian.github.io/img/avatar.jpg" />
        </div>
        <div class="name">
            <i>慎思之，笃行之</i>
        </div>
    </div>
    <div class="contents" id="nav-content">
        <ul>
            <li >
                <a href="/junlian.github.io/">
                    <i class="iconfont icon-shouye1"></i>
                    <span>主页</span>
                </a>
            </li>
            <li >
                <a href="/junlian.github.io/tags">
                    <i class="iconfont icon-biaoqian1"></i>
                    <span>标签</span>
                </a>
            </li>
            <li >
                <a href="/junlian.github.io/archives">
                    <i class="iconfont icon-guidang2"></i>
                    <span>存档</span>
                </a>
            </li>
            <li >
                <a href="/junlian.github.io/about/">
                    <i class="iconfont icon-guanyu2"></i>
                    <span>关于</span>
                </a>
            </li>
            
            <li>
                <a id="search">
                    <i class="iconfont icon-sousuo1"></i>
                    <span>搜索</span>
                </a>
            </li>
            
        </ul>
    </div>
    
        <div id="toc" class="toc-article">
    <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#深度学习推荐模型的演化关系图"><span class="toc-text">深度学习推荐模型的演化关系图</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#AutoRec-——-单隐层神经网络推荐模型"><span class="toc-text">AutoRec —— 单隐层神经网络推荐模型</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#AutoRec-模型的基本原理"><span class="toc-text">AutoRec 模型的基本原理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-2-AutoRec-模型的结构"><span class="toc-text">2.2 AutoRec 模型的结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-3-基于-AutoRec-模型的推荐过程"><span class="toc-text">2.3 基于 AutoRec 模型的推荐过程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-4-AutoRec-模型的特点和局限性"><span class="toc-text">2.4 AutoRec 模型的特点和局限性</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#3-Deep-Crossing-模型——经典的深度学习架构"><span class="toc-text">3 Deep Crossing 模型——经典的深度学习架构</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1-Deep-Crossing-模型的应用场景"><span class="toc-text">3.1 Deep Crossing 模型的应用场景</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2-Deep-Crossing-模型的网络结构"><span class="toc-text">3.2 Deep Crossing 模型的网络结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-3-Deep-Crossing-模型对特征交叉方法的革命"><span class="toc-text">3.3 Deep Crossing 模型对特征交叉方法的革命</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#4-NeuralCF-模型——CF与深度学习的结合"><span class="toc-text">4 NeuralCF 模型——CF与深度学习的结合</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#4-1-从深度学习的视角重新审视矩阵分解模型"><span class="toc-text">4.1 从深度学习的视角重新审视矩阵分解模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-2-NeuralCF-模型的结构"><span class="toc-text">4.2 NeuralCF 模型的结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#NeuralCF-模型的优势和局限性"><span class="toc-text">NeuralCF 模型的优势和局限性</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-PNN-模型——加强特征交叉能力"><span class="toc-text">5 PNN 模型——加强特征交叉能力</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-PNN-模型的网络架构"><span class="toc-text">5.1 PNN 模型的网络架构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-Product-层的多种特征交互方式"><span class="toc-text">5.2 Product 层的多种特征交互方式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-3-PNN-模型的优势和局限性"><span class="toc-text">5.3 PNN 模型的优势和局限性</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#6-Wide-amp-Deep-模型——记忆能力和泛化能力的综合"><span class="toc-text">6 Wide&amp;Deep 模型——记忆能力和泛化能力的综合</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#6-1-模型的记忆能力和泛化能力"><span class="toc-text">6.1 模型的记忆能力和泛化能力</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-2-Wide-amp-Deep-模型的结构"><span class="toc-text">6.2 Wide&amp;Deep 模型的结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-3-Wide-amp-Deep-模型的进化——Deep-amp-Cross-模型"><span class="toc-text">6.3 Wide&amp;Deep 模型的进化——Deep&amp;Cross 模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Softmax-函数"><span class="toc-text">Softmax 函数</span></a></li></ol></li></ol>
</div>
    
</div>


<div class="search-field" id="search-field">
    <div class="search-container">
        <div class="search-input">
            <span id="esc-search"> <i class="icon-fanhui iconfont"></i></span>
            <input id="search-input"/>
            <span id="begin-search">搜索</span>
        </div>
        <div class="search-result-container" id="search-result-container">

        </div>
    </div>
</div>

        <div class="index-about-mobile">
            <i> 所谓诚其意者，毋自欺也，如恶恶臭，如好好色，此之谓自谦，故君子必慎其独也。 ——摘《大学》 </i>
        </div>
    </div>
    
    <div class="index-middle">
        <!-- Main Content -->
        


<div class="post-container">
    <div class="post-title">
        《深度学习推荐系统》三 深度学习在推荐系统中的应用
    </div>

    <div class="post-meta">
        <span class="attr">发布于：<span>2022-02-11 23:27:50</span></span>
        
        <span class="attr">标签：/
        
        <a class="tag" href="/junlian.github.io/tags/#推荐系统" title="推荐系统">推荐系统</a>
        <span>/</span>
        
        
        </span>
        <span class="attr">访问：<span id="busuanzi_value_page_pv"></span>
</span>
</span>
    </div>
    <div class="post-content no-indent">
        <h1 id="深度学习推荐模型的演化关系图"><a href="#深度学习推荐模型的演化关系图" class="headerlink" title="深度学习推荐模型的演化关系图"></a>深度学习推荐模型的演化关系图</h1><p>以多层感知机为核心，演化方向包括：</p>
<ul>
<li>改变神经网络的复杂程度：增加神经网络的层数和结构复杂度，单层神经网络模型AutoRec(自编码器推荐) -&gt; 深度神经网络结构 Deep Crossing(深度特征交叉)</li>
<li>改变特征交叉方式：改变用户向量和物品向量互操作方式的 NeuralCF(神经网络协同过滤)，定义了多种特征向量交叉操作的PNN(Product-based Neural Network, 基于及操作的神经网络)模型</li>
<li>组合模型：组合不同特点，优势互补的深度学习模型，提升模型的综合能力，主要指 Wide&amp;Deep 及后续变种 Deep&amp;Cross、DeepFM 等</li>
</ul>
<h1 id="AutoRec-——-单隐层神经网络推荐模型"><a href="#AutoRec-——-单隐层神经网络推荐模型" class="headerlink" title="AutoRec —— 单隐层神经网络推荐模型"></a>AutoRec —— 单隐层神经网络推荐模型</h1><p>AutoRec 是结合自编码器的思想和协同过滤的一种单隐层神经网络推荐模型。</p>
<h2 id="AutoRec-模型的基本原理"><a href="#AutoRec-模型的基本原理" class="headerlink" title="AutoRec 模型的基本原理"></a>AutoRec 模型的基本原理</h2><p>AutoRec 模型是一个标准的自编码器，基本原理是：第一步先用协同过滤的共现矩阵，完成物品向量或用户向量的自编码；第二步再利用自编码的结果得到用户对物品的预估评分，进而进行推荐排序。</p>
<p>假设有 m 个用户，n 个物品，协同过滤的共现矩阵就是 m 个用户对 n 个物品评分形成的 $m \times n$ 维的评分矩阵。没有评分时用默认值或平均分值表示。</p>
<p>对物品 i 来说，所有 m 个用户对它的评分可表示成一个 m 维的向量 $r^{(i)}=(R_{1i},…,R_{mi})$，AutoRec 就是要构建一个重建函数 $h(r;\theta)$，使所有该重建函数生成的评分向量与原评分向量的平方残差和最小。</p>
<p>自编码器<br>自编码器是指能够完成数据自编码的模型，如将图像、音频等数据转化成向量的形式进行表达。自编码器的输入是图像、音频等数据的原始向量 $r$ ，输出向量为 $h(r, \theta)$ ，自编码过程需要使得到的输出向量尽量接近其原始向量 $r$。自编码器的目标函数为：</p>
<script type="math/tex; mode=display">min_{\theta}\sum_{r\in S}{\Vert r-h(r;\theta)\Vert}_2^2</script><p>其中 $S$ 是所有原始数据向量的集合。$h(r,\theta)$ 称为重建函数，完成训练后，重建函数 $h(r, \theta)$ 中存储了所有数据向量的精华，好处在于：1）一般重建函数的参数数量远小于输入向量的维度向量，所以自编码器相当于做了数据压缩和降维工作；2）由于经过了自编码器的泛化过程，输出向量不完全等同于输入向量，因此具备一定的缺失维度的预测能力，这也是自编码器能用于推荐系统的原因。</p>
<h2 id="2-2-AutoRec-模型的结构"><a href="#2-2-AutoRec-模型的结构" class="headerlink" title="2.2 AutoRec 模型的结构"></a>2.2 AutoRec 模型的结构</h2><p>AutoRec 构建重构函数的结构是单隐层神经网络结构。网络的输入层是物品的评分向量 $r$，输出层是多分类层，中间是 k 维的单隐层，其中 k 远小于 m。V 和 W 分别代表输入层到隐层，隐层的输出层的参数矩阵。这个模型结构下，重建函数的公式表达为：</p>
<script type="math/tex; mode=display">h(r;\theta)=f(W\cdot g(Vr+\mu)+b)</script><p>其中 $f(\cdot)$ 和 $g(\cdot)$ 分别是输出层神经元和隐层神经元的激活函数。为防止重构函数过拟合，加入 $L2$ 正则化项后的形式为：</p>
<script type="math/tex; mode=display">min_{\theta}\sum_{i=1}^n \Vert r^{(i)}-h(r^{(i)};\theta)\Vert_2^2+\frac{\lambda}{2}\cdot(\Vert W \Vert _F^2 + \Vert V \Vert _F^2)</script><p>神经元、神经网络和梯度反向传播<br>神经元，又名感知机，模型结构和逻辑回归一致。假设输入向量是$(x_1, x_2)$，先对输入进行线性加权求和，再加上一个常数偏置$b$，即变成 $(x_i\cdot w_i)+(x_2\cdot w_2)+b$；接着经过一个激活函数，激活函数的主要作用是把一个无界输入映射到一个有界的值域上。单神经元由于受到结构简单的限制，拟合能力不强，因此在解决复杂问题时，经常会用多神经元组成一个网络，使之具备拟合任何复杂函数的能力，即神经网络。经过对神经元不同连接方式的探索，才衍生出各种不同特性的深度学习网络。</p>
<p>清楚了神经网络的模型结构后，如何训练呢，就需要用到前向传播和反向传播。前向传播的目的是在当前网络参数的基础上得到模型对输入的预估值，即模型推断过程。在得到预估值后，就可以利用损失函数的定义计算模型的损失。再利用基于链式法则的梯度反向传播来更新权重训练神经网络</p>
<h2 id="2-3-基于-AutoRec-模型的推荐过程"><a href="#2-3-基于-AutoRec-模型的推荐过程" class="headerlink" title="2.3 基于 AutoRec 模型的推荐过程"></a>2.3 基于 AutoRec 模型的推荐过程</h2><p>输入物品 i 的评分向量为 $r^{(i)}$，模型的输出向量 $h(r^{(i)}, \theta)$ 就是所有用户对物品 i 的评分预测，其中第 u 维就是用户 u 对物品 i 的预测 $\check{R}_{ui}$:</p>
<script type="math/tex; mode=display">\check{R}_{ui} = (h(r^{(i)}, \check{\theta}))_u</script><p>通过便利输入物品向量就可以得到用户 u 对所有物品的评分预测，进而根据评分预测排序得到推荐列表。</p>
<p>和协同过滤算法一样，AutoRec 也分基于物品的 I-AutoRec（输入向量是物品的评分向量）和基于用户的 U-AutoRec(输入向量是用户的评分向量）。U-AutoRec 得到的优势是在生成推荐列表过程中，只需输入一次目标用户的用户向量，就可以重建用户对所有物品的评分向量，即只需一次模型推断过程就能得到用户的推荐列表。劣势是用户向量的稀疏性可能会影响模型效果。</p>
<h2 id="2-4-AutoRec-模型的特点和局限性"><a href="#2-4-AutoRec-模型的特点和局限性" class="headerlink" title="2.4 AutoRec 模型的特点和局限性"></a>2.4 AutoRec 模型的特点和局限性</h2><p>优点：1）拉开了使用深度学习解决推荐问题的序幕；2）用单隐层的AutoEncoder泛化用户和物品评分，使模型有一定的泛化和表达能力<br>局限：模型结构简单，存在表达能力不足的问题。</p>
<p>AutoRec 结构和词向量模型（word2vec）完全一致，单优化目标和训练方法有所不同。</p>
<h1 id="3-Deep-Crossing-模型——经典的深度学习架构"><a href="#3-Deep-Crossing-模型——经典的深度学习架构" class="headerlink" title="3 Deep Crossing 模型——经典的深度学习架构"></a>3 Deep Crossing 模型——经典的深度学习架构</h1><h2 id="3-1-Deep-Crossing-模型的应用场景"><a href="#3-1-Deep-Crossing-模型的应用场景" class="headerlink" title="3.1 Deep Crossing 模型的应用场景"></a>3.1 Deep Crossing 模型的应用场景</h2><p>应用场景是微软搜索引擎 Bing 中的搜索广告推荐场景，用户在搜索引擎中输入搜索词之后，搜索引擎除了会返回相关结果，还会返回与搜索词相关的广告，这也是大多数搜索引擎的主要盈利模式。Deep Crossing 的优化目标是准确地预测广告点击率，以此作为广告排序的指标之一。</p>
<p>使用的特征分三类：1）可以被处理成 one-hot 或 multi-hot 向量的类别型特征，如用户搜索词，广告关键词，广告标题，落地页，匹配类型等；2）数值型特征/计数型Counting特征，如点击率，预估点击率等；3）需要进一步处理的特征，如广告计划（广告主创建的广告投放计划，包括预算、定向条件等），曝光样例（记录了广告在实际曝光场景中的相关信息），点击样例等。</p>
<p>类别型特征可以通过one-hot或multi-hot编码生成特征向量，数值型特征则可以直接拼接进特征向量中。</p>
<h2 id="3-2-Deep-Crossing-模型的网络结构"><a href="#3-2-Deep-Crossing-模型的网络结构" class="headerlink" title="3.2 Deep Crossing 模型的网络结构"></a>3.2 Deep Crossing 模型的网络结构</h2><p>深度学习网络的特点是可以根据需求灵活地对网络结构进行调整，从而达成从原始特征向量到最终的优化目标的端到端的训练目的。为了完成端到端的训练，Deep Crossing 需要解决以下问题；</p>
<ul>
<li>如何解决稀疏特征向量稠密化问题：离散类特征编码后过于稀疏，不利于直接输入神经网络进行训练</li>
<li>如何解决特征自动交叉组合的问题</li>
<li>如何在输出层达成问题设定的优化目标</li>
</ul>
<p>Deep Crossing 模型分别设置了不同的神经网络层来解决这三个问题，其网络结构包括4层：Embedding 层，Stacking 层，Muiltiple Residual Units 层和 Score 层</p>
<p>Embedding 层</p>
<ul>
<li>作用：将稀疏的类别型特征转化成稠密的embedding向量。每一个经one-hot编码后的稀疏特征向量，经过embedding层后，会转换成对应的embedding向量。数值型特征不需要经过Embedding层，直接进入 Stacking 层。</li>
<li>结构：以全连接层Fully Connected Layer 结构为主，还有 Word2vec、Graph Embedding 等多种不同的Embedding方法。</li>
<li>特点：Embedding 向量的维度应远小于原始的稀疏特征向量，几十到上百维一般就能满足需求。</li>
</ul>
<p>Stacking 层/ 连接(Concatenate)层</p>
<ul>
<li>作用：把不同的 Embedding 特征和数值型特征拼接在一起，形成新的包含全部特征的特征向量</li>
</ul>
<p>Multiple Residual Units 层</p>
<ul>
<li>作用：通过多层残差网络对特征向量各个维度进行充分的交叉组合，使模型能够抓取到更多的非线性特征和组合特征的信息，增大模型表达能力。</li>
<li>结构：主要结构是多层感知机，Deep Crossing 模型采用了多层残差网络作为MLP的具体实现。</li>
</ul>
<p>Scoring 层</p>
<ul>
<li>作用：输出层，训练时用于拟合优化目标。</li>
<li>结构：CTR预估这类二分类问题往往使用逻辑回归模型，多分类问题往往用softmax模型</li>
</ul>
<blockquote>
<p>残差神经网络<br>残差神经网络就是由残差单元组成的神经网络。相比传统感知机，残差单元结构上主要有两个不同：</p>
<ul>
<li>输入经过两层以ReLU为激活函数的全连接层后，生成输出向量</li>
<li>输入可以通过一个短路（Shortcut）通路直接与输出向量进行元素加（element-wise plus）操作，生成最终的输出向量<br>这种结构下，残差单元中的两层 ReLU 其实拟合的就是输出和输入之间的残差，即残差神经网络名称的由来。<br>设计残差神经网络主要是为了解决两个问题：</li>
<li>减少过拟合：基于传统感知机的神经网络，当网络加深后往往会出现过拟合，即在测试集上表现越差。残差神经网络中由于存在输入向量短路，很多时候可以越过两层ReLU网络，减少过拟合现象发生。</li>
<li>避免梯度消失，加快收敛速度：当网络加深后往往存在严重的梯度消失现象（在梯度反向传播过程中，越靠近输入端，梯度的幅度越小，参数收敛的速度越慢）。残差单元用ReLU激活函数取代sigmoid激活函数，输入向量短路相当于直接把梯度毫无变化地传递到下一层，都是的残差网络的手链速度更快。    </li>
</ul>
</blockquote>
<h2 id="3-3-Deep-Crossing-模型对特征交叉方法的革命"><a href="#3-3-Deep-Crossing-模型对特征交叉方法的革命" class="headerlink" title="3.3 Deep Crossing 模型对特征交叉方法的革命"></a>3.3 Deep Crossing 模型对特征交叉方法的革命</h2><ul>
<li>没有任何人工特征工程的参与</li>
<li>相比 FM，FFM 只具备二阶特征交叉的能力，可以通过调整神经网络的深度进行特征间的深度交叉</li>
</ul>
<h1 id="4-NeuralCF-模型——CF与深度学习的结合"><a href="#4-NeuralCF-模型——CF与深度学习的结合" class="headerlink" title="4 NeuralCF 模型——CF与深度学习的结合"></a>4 NeuralCF 模型——CF与深度学习的结合</h1><p>NeuralCF 的主要思想是利用多层神经网络替代经典协同过滤的积操作。广义上，任何向量之间的交互计算方式都可以用来代替协同过滤的内积操作，相应的模型可称为广义的矩阵分解模型。<br>推荐系统的经典算法沿着协同过滤的思路，发展出了矩阵分解技术，将协同过滤中的共现矩阵分解为用户向量矩阵和物品向量矩阵，用户u的隐向量和物品i的隐向量的内积，就是用户u对物品i的评分预测。NueralCF则是沿着矩阵分解的技术脉络，结合了深度学习而提出的模型。</p>
<h2 id="4-1-从深度学习的视角重新审视矩阵分解模型"><a href="#4-1-从深度学习的视角重新审视矩阵分解模型" class="headerlink" title="4.1 从深度学习的视角重新审视矩阵分解模型"></a>4.1 从深度学习的视角重新审视矩阵分解模型</h2><p>如果深度学习的网络图来描述矩阵分解模型，矩阵分解层的用户隐向量和物品隐向量可以看作一种Embedding方法。Scoring 层就是将用户隐向量和物品隐向量进行内积操作后得到的”相似度”。</p>
<p>由于矩阵分解模型结构相对简单，特别是输出层/Scoring层无法对优化目标进行有效拟合，实际使用时往往处于欠拟合状态。为了让模型有更强的表达能力，新加坡国立大学提出了 NeuralCF 模型。</p>
<h2 id="4-2-NeuralCF-模型的结构"><a href="#4-2-NeuralCF-模型的结构" class="headerlink" title="4.2 NeuralCF 模型的结构"></a>4.2 NeuralCF 模型的结构</h2><p>NeuralFC 用多层神经网络+输出层的结构替代了矩阵分解模型中简单的内积操作。收益是：1）让用户向量和物品向量做更充分的交叉，得到更多有价值的特征组合信息；2）引入更多非线性特征，加强模型的表达能力。</p>
<p>事实上，用户和物品向量的互操作层可以被任意的互操作形式替代，即“广义矩阵分解”模型。原始的矩阵分解用内积的方式让用户和物品向量交互，为了进一步让向量在各维度上进行充分交叉，可以通过”元素积“（element-wise product）将长度相同的两个向量的对应维相乘的到另一向量，再通过逻辑回归等输出层拟合最终的预测目标。NeuralCF中是利用神经网络拟合互操作函数，是广义的互操作形式。还有更多的互操作形式，见PNN模型、Deep&amp;Cross模型。</p>
<p>再进一步，可以把通过不同互操作网络得到的特征向量拼接起来，交给输出层进行目标拟合。NeuralCF混合模型就是整合了原始的NeuralCF模型和以元素积为互操作的广义矩阵分解模型，让模型具有更强的特征组合和非线性能力。</p>
<h2 id="NeuralCF-模型的优势和局限性"><a href="#NeuralCF-模型的优势和局限性" class="headerlink" title="NeuralCF 模型的优势和局限性"></a>NeuralCF 模型的优势和局限性</h2><p>优势：提出了一个模型框架，基于用户和物品向量两个embedding层，利用不同的互操作层进行特征的交叉组合，并且可以灵活地进行不同互操作层的拼接。从这里可以看出深度学习构建推荐模型的优势，即利用神经网络理论上可以拟合任何函数的能力，灵活地组合不同特征，按需增加或减少模型的复杂度。<br>局限：1）由于仍是基于协同过滤的思想，所以NeuralCF并没有引入更多其他有价值的特征信息。2）互操作的种类有进一步探究的空间。</p>
<h2 id="5-PNN-模型——加强特征交叉能力"><a href="#5-PNN-模型——加强特征交叉能力" class="headerlink" title="5 PNN 模型——加强特征交叉能力"></a>5 PNN 模型——加强特征交叉能力</h2><p>NeuralCF 模型只用了用户向量和物品向量两组特征向量，如果加入多组特征向量又该如何设计特征交互的方法呢，PNN 模型就是给出了特征交互方式的几种设计思路。</p>
<h3 id="5-1-PNN-模型的网络架构"><a href="#5-1-PNN-模型的网络架构" class="headerlink" title="5.1 PNN 模型的网络架构"></a>5.1 PNN 模型的网络架构</h3><p>PNN 模型的输入、Embedding 层、多层神经网络和最终的输出层在结构上都和 Deep Crossing 模型相同，唯一的区别在PNN模型用乘积层（Product Layer）代替了Deep Crossing 模型中的 Stacking 层，即不同特征的 Embedding 向量不再是简单的拼接，而是用 Product 操作进行两两交互，更有针对性地获取特征之间的交叉信息。</p>
<p>相比 NeuralCF，PNN 的输入不仅包括用户和物品信息，还可以有更多其他特征，通过 Embedding 层的编码生成长度相同的稠密特征 Embedding 向量。除此之外 PNN 还给出了更多针对特征交叉的具体互操作方式。</p>
<h3 id="5-2-Product-层的多种特征交互方式"><a href="#5-2-Product-层的多种特征交互方式" class="headerlink" title="5.2 Product 层的多种特征交互方式"></a>5.2 Product 层的多种特征交互方式</h3><p>PNN 结构的创新在于乘积层的引入。PNN 乘积层由线性操作部分（对各特征向量进行线性拼接）和乘积操作部分组成。其中乘积特征交叉部分又分为内积操作和外积操作，使用内积操作的 PNN 被称为 IPNN（Inner Product-base Neural Network），使用外积操作的 PNN 称为 OPNN（Outer Product-based Neural Network）。无论内积还是外积，都是对不同特征的 Embedding 向量进行两两组合。这就要求各 Embedding 向量的维度必须相同。</p>
<p>内积操作就是向量内积运算，生成的是一个向量。外积操作是对两个特征向量的各维度进行两两交叉 $g(f_i,f_j)=f_if_j^T$，生成的是一个 $M\times M$ 的方形矩阵，如果输入向量的维度是 M，问题的复杂度会从 M 提升到 $M^2$。为了减小模型训练负担，可以采用降维的方法，就是把所有两两特征 Embedding 向量外积互操作的结果叠加，形成一个叠加外籍互操作矩阵 $p$:</p>
<script type="math/tex; mode=display">p=\sum_{i=1}^N\sum_{j=1}^N f_if_j^T=f_{\sum}f_{\sum}^T,f_{\sum}=\sum_{i=1}^N f_i</script><p>叠加矩阵 $p$ 的最终形式类似于让所有特征 Embedding 向量通过一个平均池化层后，再进行外积互操作。</p>
<p>平均池化操作需要谨慎使用。把不同特征对应维度进行平均，实际上是假设不同特征的对应维度有类似的含义，平均池化操作通常用在同类 Embedding 上，比如将用户浏览过的多个视频的 Embedding 进行平均。而如果一个特征是年龄，一个是地域，那这两个特征在经过各自的 Embedding 层后，二者的 Embedding 向量不在一个向量空间中，显然不具有任何可比性。这时把二者平均会模糊很多有价值的信息。</p>
<p>PNN 模型经过特征的线性和乘积操作后，在乘积层内部又进行了局部全连接层的转换，分别将线性部分和乘积部分映射成了 $D_1$ 维($L_1$ 隐层的神经元数量)的输入向量 $l_z$ 和 $l_p$，再将 $l_z$ 和 $l_p$ 叠加，输入 $L_2$ 隐层。</p>
<h3 id="5-3-PNN-模型的优势和局限性"><a href="#5-3-PNN-模型的优势和局限性" class="headerlink" title="5.3 PNN 模型的优势和局限性"></a>5.3 PNN 模型的优势和局限性</h3><p>优势：</p>
<ul>
<li>强调了 Embedding 向量之间的交叉方式是多样化的，相比直接交由全连接层进行无差别处理，PNN 的内积和外积操作显然更有针对性地强调了不同特征之间的交互，让模型更容易捕获交叉特征。<br>局限：</li>
<li>外积操作中为了优化训练交互用平均池化进行了大量简化，损失了很多信息</li>
<li>对所有特征无差别的交叉，一定程度上忽略了原始特征向量中包含的有价值信息。</li>
</ul>
<p>如何综合原始特征和交叉特征，让特征交叉的方式更高效，后续的 Wide&amp;Deep 模型和基于 FM 的各类深度学习模型将给出它们的解决方案</p>
<h1 id="6-Wide-amp-Deep-模型——记忆能力和泛化能力的综合"><a href="#6-Wide-amp-Deep-模型——记忆能力和泛化能力的综合" class="headerlink" title="6 Wide&amp;Deep 模型——记忆能力和泛化能力的综合"></a>6 Wide&amp;Deep 模型——记忆能力和泛化能力的综合</h1><p>Wide&amp;Deep 是由单层的 Wide 部分和多层的 Deep buff组成的混合模型。其中 Wide 部分的主要作用是让模型具有较强的记忆能力，Deep 部分主要作用是让模型具有泛化能力。这样的结构特点，使模型兼具了逻辑回归和深度神经网络的优点——能快速处理并记忆大量历史行为特征，并具有强大的表达能力。</p>
<h2 id="6-1-模型的记忆能力和泛化能力"><a href="#6-1-模型的记忆能力和泛化能力" class="headerlink" title="6.1 模型的记忆能力和泛化能力"></a>6.1 模型的记忆能力和泛化能力</h2><p>记忆能力<br>可以理解为模型直接学习并利用历史数据中的物品或特征的”共现频率”的能力。协同过滤、逻辑回归等简单模型有较强的记忆能力。因为结构简单，原始数据往往可以直接影响推荐结构，产生类似于“如果点击过A，就推荐B”这类规则式的推荐，相当于模型直接记住了历史数据的分布特点，并利用这些记忆进行推荐。<br>假设有组合特征AND(安装netfilx, 曾在应用商店看过pandora应用)，label 为最终是否安装pandora。首先统计出 netfilx&amp;pandora 这个特征和安装 pandora 这个 label 之间的共现频率，假设二者的共现频率高达 10%，而全局的平均应用安装率为 1%，这个特征如此之强，以至于在设计模型时，希望模型一发现有这个特征，就推荐 pandora 这款应用，就像一个深刻的记忆点一样印在脑子里，这就是所谓的模型的记忆能力。<br>像逻辑回归这类简单模型，如果发现这样的强特征，器相应的权重就会在模型训练过程中被调整地非常大，这样就实现了对这个特征的直接记忆。相反，对于多层神经网络来说，特征会被多层处理，不断与其他特征进行交叉，因此模型对强特征的记忆反而没有简单模型深刻</p>
<p>泛化能力<br>可以理解为模型传递特征的相关性，以及发掘稀疏甚至从未出现过的稀疏特征与最终标签相关性的能力<br>矩阵分解比协同过滤的泛化能力强，因为其引入了隐向量这样的结构，使得数据稀少的用户或物品也能生成隐向量，从而获得有数据支撑的推荐得分，这是非常典型的将全局数据传递到稀疏物品上，从而提高泛化能力的例子。<br>深度神经网络通过特征的多次自动组合，可以深度发掘数据中潜在的模式，即使时非常稀疏的特征向量输入，也能得到较稳定平滑的推荐概率，这就是简单模型所缺乏的泛化能力</p>
<h2 id="6-2-Wide-amp-Deep-模型的结构"><a href="#6-2-Wide-amp-Deep-模型的结构" class="headerlink" title="6.2 Wide&amp;Deep 模型的结构"></a>6.2 Wide&amp;Deep 模型的结构</h2><p>Wide&amp;Deep 模型把单输入层的 Wide 部分与由 Embedding 层和多隐层组成的 Deep 部分连接起来，一起输入最终的逻辑回归输出层，完成 Wide 和 Deep 的融合。</p>
<p>单层的 Wide 部分善于处理大量稀疏的 id 类特征：</p>
<ul>
<li>输入：只有已安装应用（代表用户历史行为）和曝光应用（代表当前的待推荐应用）两类特征。简单模型善于记忆用户行为特征中的信息，并根据此信息直接影响推荐结果，对这两类特征充分发挥Wide部分记忆能力强的优势。 </li>
<li>结构：通过交叉积变换层完成特征组合，再将组合特征输入最终的 LogLoss 输出层。交叉积变换层的形式为：<script type="math/tex; mode=display">\Phi_K(X)=\prod _{i=1}^{d} x_i^{c_{ki}} c_{ki}\in\{0,1\}</script>$c_{ki}$ 是一个布尔变量，当第 i 个特征属于第 k 个组合特征时，$c_{ki}$ 的值为1，否则为 0，$x_i$ 是第 i 个特征的值。这种形式下，对于AND(user_installed_app=netflix, impression_app=pandora) 这个组合特征来说，只有当 user_installed_app=netflix 和 impression_app=pandora 这两个特征同时为 1 时，其对应的交叉积变换层的结果才为 1，否则为 0。</li>
</ul>
<p>Deep 部分进行深层的特征交叉，挖掘藏在特征背后的数据模式。</p>
<ul>
<li>输入：全量的特征向量，包括用户年龄，已安装应用数量，设备类型，已安装应用，曝光应用等特征。</li>
<li>结构：已安装应用等类别型特征需要先经过 Embedding 层，再和其他特征一起输入连接层，拼接成 1200 维的 Embedding 向量。依次经过 3 层 ReLU 全连接层（1024，512，256），最终输入 LogLoss 输出层。</li>
</ul>
<h2 id="6-3-Wide-amp-Deep-模型的进化——Deep-amp-Cross-模型"><a href="#6-3-Wide-amp-Deep-模型的进化——Deep-amp-Cross-模型" class="headerlink" title="6.3 Wide&amp;Deep 模型的进化——Deep&amp;Cross 模型"></a>6.3 Wide&amp;Deep 模型的进化——Deep&amp;Cross 模型</h2><h2 id="Softmax-函数"><a href="#Softmax-函数" class="headerlink" title="Softmax 函数"></a>Softmax 函数</h2><p>Softmax 函数的数学形式定义<br>给定一个 n 维向量，softmax 函数将其映射为一个概率分布：</p>
<script type="math/tex; mode=display">\sigma(X)_i = \frac{exp(x_i)}{\sum_{j=1}^n exp(x_j)}</script><p>在多分类问题中，假设分类数是n，那么深度学习模型最后输出层的形式为 n 个神经元，把 n 个神经元的输出结果作为 n 维向量输入最终的 softmax 函数，得到最终的多分类概率分布。分类问题中 Softmax 通常和交叉上损失函数一起使用：</p>
<script type="math/tex; mode=display">Loss_{cross entropy} = -\sum_i y_iln(\sigma(x)_i)</script><p>其中 $y_i$ 是第 i 个分类的真实标签值，$\sigma (x)_i$ 代表softmax函数对第 i 个分类的预测值。</p>
<p>softmax+交叉熵，不仅数学含义上完美统一（softmax函数把分类输出标准化成了多个分类的概率分布，而交叉熵正好刻画了预测分类和真实分类之间的相似度），梯度形式上也非常简单：</p>
<ul>
<li>补充 梯度链式公式</li>
</ul>

        
        <br />
        <div id="comment-container">
        </div>
        <div id="disqus_thread"></div>

        <div id="lv-container">
        </div>

    </div>
</div>

    </div>
</div>


<footer class="footer">
    <ul class="list-inline text-center">
        
        

        

        

        

        

    </ul>
    
    <p>
        <span id="busuanzi_container_site_pv">
            <span id="busuanzi_value_site_pv"></span>PV
        </span>
        <span id="busuanzi_container_site_uv">
            <span id="busuanzi_value_site_uv"></span>UV
        </span>
        Created By <a href="https://hexo.io/">Hexo</a>  Theme <a href="https://github.com/aircloud/hexo-theme-aircloud">AirCloud</a></p>
</footer>




<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });

</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<!-- <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> -->

</body>

<script>
    // We expose some of the variables needed by the front end
    window.hexo_search_path = "search.json"
    window.hexo_root = "/junlian.github.io/"
    window.isPost = true
</script>
<script src="https://cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script>
<script src="/junlian.github.io/js/index.js"></script>
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




</html>
